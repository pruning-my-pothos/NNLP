{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini RAG Eval (Local, Toy Data)\n",
    "Quick recall@k check on a tiny corpus. Replace the toy docs with your chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "docs = [\n",
    "    (\"doc1\", \"The sky is blue because molecules scatter sunlight.\"),\n",
    "    (\"doc2\", \"Plants make food through photosynthesis using sunlight.\"),\n",
    "    (\"doc3\", \"HTTP GET retrieves data; POST sends data to the server.\"),\n",
    "]\n",
    "corpus = [t for _, t in docs]\n",
    "ids = [d for d, _ in docs]\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "doc_vecs = vec.fit_transform(corpus)\n",
    "\n",
    "def top_k(query, k=2):\n",
    "    qv = vec.transform([query])\n",
    "    scores = cosine_similarity(qv, doc_vecs).flatten()\n",
    "    order = scores.argsort()[::-1][:k]\n",
    "    return [(ids[i], scores[i]) for i in order]\n",
    "\n",
    "test_set = [\n",
    "    (\"why is the sky blue\", [\"doc1\"]),\n",
    "    (\"how do plants make food\", [\"doc2\"]),\n",
    "    (\"what is http get\", [\"doc3\"]),\n",
    "]\n",
    "\n",
    "def hit_rate(k=2):\n",
    "    hits = 0\n",
    "    for q, expected in test_set:\n",
    "        retrieved = [doc_id for doc_id, _ in top_k(q, k)]\n",
    "        if any(e in retrieved for e in expected):\n",
    "            hits += 1\n",
    "    rate = hits / len(test_set)\n",
    "    print(f\"Hit rate @{k}: {rate:.2f} ({hits}/{len(test_set)})\")\n",
    "\n",
    "for k in (1, 2, 3):\n",
    "    hit_rate(k)\n",
    "\n",
    "print(top_k(\"why is the sky blue\", k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to adapt\n",
    "- Swap `docs` with your chunk texts and IDs.\n",
    "- Replace TF-IDF with your embedding store for realism.\n",
    "- Expand `test_set` with real (query, expected_ids) pairs.\n",
    "- Add citation/faithfulness checks for a full RAG eval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
