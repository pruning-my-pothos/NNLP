"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[5746],{63164(e){e.exports=JSON.parse('{"tag":{"label":"risk","permalink":"/gen-ai-llm-docs/docs/tags/risk","allTagsPath":"/gen-ai-llm-docs/docs/tags","count":5,"items":[{"id":"04-responsible-ai/guardrails-index","title":"Guardrails and Governance Index","description":"Guardrails and Governance define the protective measures and policies necessary to ensure AI is used safely, ethically, and in compliance with organizational and regulatory standards. These are critical for managing the risks inherent in AI-assisted development.","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/guardrails-index"},{"id":"04-responsible-ai/02-hallucinations","title":"Hallucinations and Failure Modes","description":"Identify common failures, why they occur, and how the methods in this documentation help mitigate them. Understanding these modes is crucial for building reliable AI-assisted workflows and prevents over-reliance on AI.","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/02-hallucinations"},{"id":"foundations/02-llm-deep-dive/fundamentals/07-hallucinations-and-failure-modes","title":"Hallucinations and Failure Modes","description":"Identify common failures, why they occur, and how GenAI & LLM Documentation mitigates them. Understanding these modes is crucial for building reliable AI-assisted workflows and prevents over-reliance on AI.","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/fundamentals/07-hallucinations-and-failure-modes"},{"id":"06-templates/threat-model-lite-template","title":"Template: Threat Model Lite","description":"Fill this out for any feature that uses an LLM. If you answer \\"Yes\\" to any High Risk item, you must have a specific mitigation listed.","permalink":"/gen-ai-llm-docs/docs/06-templates/threat-model-lite-template"},{"id":"04-responsible-ai/threat-model-lite","title":"Threat Model Lite","description":"Quickly identify and mitigate security risks associated with AI-assisted development. This lightweight threat modeling approach focuses on AI-specific vulnerabilities and ensures that security considerations are embedded early in the GenAI & LLM Documentation Loop.","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/threat-model-lite"}],"unlisted":false}}')}}]);