"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[934],{28453(e,n,i){i.d(n,{R:()=>l,x:()=>r});var t=i(96540);const a={},s=t.createContext(a);function l(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),t.createElement(s.Provider,{value:n},e.children)}},48797(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"04-responsible-ai/guardrails-index","title":"Guardrails and Governance Index","description":"Guardrails and Governance define the protective measures and policies necessary to ensure AI is used safely, ethically, and in compliance with organizational and regulatory standards. These are critical for managing the risks inherent in AI-assisted development.","source":"@site/../docs/04-responsible-ai/guardrails-index.md","sourceDirName":"04-responsible-ai","slug":"/04-responsible-ai/guardrails-index","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/guardrails-index","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"genai-llm","permalink":"/gen-ai-llm-docs/docs/tags/genai-llm"},{"inline":true,"label":"governance","permalink":"/gen-ai-llm-docs/docs/tags/governance"},{"inline":true,"label":"safety","permalink":"/gen-ai-llm-docs/docs/tags/safety"},{"inline":true,"label":"risk","permalink":"/gen-ai-llm-docs/docs/tags/risk"},{"inline":true,"label":"compliance","permalink":"/gen-ai-llm-docs/docs/tags/compliance"}],"version":"current","lastUpdatedAt":null,"frontMatter":{"title":"Guardrails and Governance Index","archetype":"guardrail-index","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["genai-llm","governance","safety","risk","compliance"],"last_reviewed":"2025-12-28"},"sidebar":"mainSidebar","previous":{"title":"Explainable AI","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/explainable-ai"},"next":{"title":"Threat Model Lite","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/threat-model-lite"}}');var a=i(74848),s=i(28453);const l={title:"Guardrails and Governance Index",archetype:"guardrail-index",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["genai-llm","governance","safety","risk","compliance"],last_reviewed:"2025-12-28"},r="Guardrails and Governance Index",o={},d=[{value:"Overview",id:"overview",level:2},{value:"When to Use",id:"when-to-use",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Guardrail Categories",id:"key-guardrail-categories",level:2},{value:"1. Data Boundaries",id:"1-data-boundaries",level:3},{value:"2. Accountability Model",id:"2-accountability-model",level:3},{value:"3. Threat Modeling",id:"3-threat-modeling",level:3},{value:"4. Human Review Protocols",id:"4-human-review-protocols",level:3},{value:"5. Automated Policy Enforcement",id:"5-automated-policy-enforcement",level:3},{value:"6. Transparency and Explainability",id:"6-transparency-and-explainability",level:3},{value:"Visual Summary of Guardrails in the GenAI &amp; LLM Documentation Loop",id:"visual-summary-of-guardrails-in-the-genai--llm-documentation-loop",level:2},{value:"Common Pitfalls",id:"common-pitfalls",level:2},{value:"Last Reviewed / Last Updated",id:"last-reviewed--last-updated",level:2}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"guardrails-and-governance-index",children:"Guardrails and Governance Index"})}),"\n",(0,a.jsx)(n.admonition,{title:"Value Proposition",type:"info",children:(0,a.jsx)(n.p,{children:"Guardrails and Governance define the protective measures and policies necessary to ensure AI is used safely, ethically, and in compliance with organizational and regulatory standards. These are critical for managing the risks inherent in AI-assisted development."})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(n.p,{children:["In GenAI & LLM Documentation, we do not treat AI as a magic box. We treat it as a ",(0,a.jsx)(n.strong,{children:"junior employee with infinite speed and zero judgment"}),'. Without proper guardrails and governance, this "junior employee" can inadvertently introduce significant risks, from security vulnerabilities and data leakage to compliance violations and brand damage. This section provides a framework for establishing the necessary controls to manage AI-assisted development effectively.']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Goal"}),": Establish clear policies, processes, and technical controls to ensure responsible and reliable AI integration.\n",(0,a.jsx)(n.strong,{children:"Anti-pattern"}),": Allowing ad-hoc AI usage without any oversight, leading to unpredictable risks and inconsistent quality."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"when-to-use",children:"When to Use"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{style:{textAlign:"left"},children:"\u2705 Use This Pattern When..."}),(0,a.jsx)(n.th,{style:{textAlign:"left"},children:"\ud83d\udeab Do Not Use When..."})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Integrating AI into any production workflow"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"You are purely experimenting in a sandboxed, isolated environment"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Handling sensitive data with AI"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"The AI functionality is purely passive (e.g., simple text summarization of public data)"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Ensuring legal and ethical compliance for AI outputs"}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"You want to delegate responsibility for safety and compliance solely to the AI tool provider"})]})]})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.admonition,{title:"Before you start",type:"warning",children:(0,a.jsx)(n.p,{children:"A clear understanding of your organization's risk appetite, compliance obligations, and ethical guidelines for AI use is essential."})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Artifacts"}),": Organizational AI policies (if available)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Context"}),": Awareness of the specific risks associated with your industry and data."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"key-guardrail-categories",children:"Key Guardrail Categories"}),"\n",(0,a.jsx)(n.h3,{id:"1-data-boundaries",children:"1. Data Boundaries"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focus"}),": Controlling what data (code, PII, proprietary information) can be exposed to which AI tools."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mitigates"}),": Data leakage, privacy violations, IP theft."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"See"}),": ",(0,a.jsx)(n.code,{children:"docs/07-guardrails/data-boundaries.md"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-accountability-model",children:"2. Accountability Model"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focus"}),": Clearly defining human roles, responsibilities, and ownership for AI-assisted work."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mitigates"}),": Blaming the AI, lack of ownership for AI-generated errors."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"See"}),": ",(0,a.jsx)(n.code,{children:"docs/07-guardrails/accountability-model.md"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-threat-modeling",children:"3. Threat Modeling"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focus"}),": Proactively identifying AI-specific security vulnerabilities (e.g., prompt injection, data poisoning)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mitigates"}),": Security breaches, exploitation of AI-generated code."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"See"}),": ",(0,a.jsx)(n.code,{children:"docs/07-guardrails/threat-model-lite.md"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-human-review-protocols",children:"4. Human Review Protocols"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focus"}),": Standardized processes for reviewing and interrogating AI outputs for correctness, safety, and compliance."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mitigates"}),": Accidental deployment of flawed or harmful AI outputs."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"See"}),": ",(0,a.jsx)(n.code,{children:"docs/08-evaluation/03-human-review-protocols.md"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"5-automated-policy-enforcement",children:"5. Automated Policy Enforcement"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focus"}),": Using automated tools (linters, static analysis, CI/CD checks) to enforce rules and detect violations in AI-generated code."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mitigates"}),": Compliance drift, introduction of unidiomatic or insecure code."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"See"}),": ",(0,a.jsx)(n.code,{children:"docs/08-evaluation/02-automated-evaluation.md"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"6-transparency-and-explainability",children:"6. Transparency and Explainability"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Focus"}),": Documenting AI decisions and outputs, understanding AI rationale."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mitigates"}),': "Black box" AI behavior, difficulty in auditing.']}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"visual-summary-of-guardrails-in-the-genai--llm-documentation-loop",children:"Visual Summary of Guardrails in the GenAI & LLM Documentation Loop"}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TD\n    subgraph Planning\n        Intent(Intent Spec)\n        Constraints(Constraint Spec)\n    end\n    subgraph Execution ["\u2699\ufe0f GenAI & LLM Documentation Loop"]\n        Delegation(Delegation Contract)\n        Generation(Generation Request)\n        Review(Review & Interrogation)\n        Acceptance(Acceptance Criteria)\n    end\n    subgraph Oversight\n        Accountability(Accountability Model)\n        Data(Data Boundaries)\n        Threat(Threat Model Lite)\n        Human(Human Review Protocols)\n        Auto(Automated Policy Enforcement)\n    end\n\n    Intent --\x3e Delegation\n    Constraints --\x3e Delegation\n    Delegation --\x3e Generation\n    Generation --\x3e Review\n    Review --\x3e Acceptance\n\n    Planning --\x3e Oversight\n    Execution --\x3e Oversight'}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"common-pitfalls",children:"Common Pitfalls"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{style:{textAlign:"left"},children:"Pitfall"}),(0,a.jsx)(n.th,{style:{textAlign:"left"},children:"Impact"}),(0,a.jsx)(n.th,{style:{textAlign:"left"},children:"Correction"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"Over-engineering Guardrails"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Slows down development, stifles AI adoption."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Start with lightweight, high-impact guardrails and iterate."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:'"Set It and Forget It"'})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Guardrails become outdated as AI evolves."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Regularly review and adapt guardrails based on new risks and tool capabilities."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{style:{textAlign:"left"},children:(0,a.jsx)(n.strong,{children:"Ignoring Human Factors"})}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Policies are not adopted by the team."}),(0,a.jsx)(n.td,{style:{textAlign:"left"},children:"Involve team members in developing guardrails and ensure clear communication."})]})]})]}),"\n",(0,a.jsx)(n.admonition,{title:"Critical Risk",type:"danger",children:(0,a.jsx)(n.p,{children:"Guardrails are living documents. Without continuous review and adaptation, they can provide a false sense of security."})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"last-reviewed--last-updated",children:"Last Reviewed / Last Updated"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Last reviewed: 2025-12-28"}),"\n",(0,a.jsx)(n.li,{children:"Version: 0.1.0"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);