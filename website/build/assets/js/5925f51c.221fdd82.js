"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[7581],{85180(e){e.exports=JSON.parse('{"tag":{"label":"evaluation","permalink":"/gen-ai-llm-docs/docs/tags/evaluation","allTagsPath":"/gen-ai-llm-docs/docs/tags","count":6,"items":[{"id":"01-handbook-core-method/08-evaluation/02-automated-evaluation","title":"Automated Evaluation","description":"In GenAI & LLM Documentation, we treat evaluation as a pipeline, not just a meeting. Configure your CLI agent (e.g., Aider) to run these checks before asking for human review.","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/02-automated-evaluation"},{"id":"01-handbook-core-method/08-evaluation/00-eval-overview","title":"Evaluation Overview","description":"Evaluation shifts \\"looks good\\" to \\"is good,\\" providing objective evidence that AI-assisted work meets quality and performance criteria.","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/00-eval-overview"},{"id":"01-handbook-core-method/08-evaluation/03-human-review-protocols","title":"Human Review Protocols","description":"Systematically verify AI-generated outputs against your Intent and Constraint Specs. This is where human judgment ensures correctness, safety, and alignment before acceptance.","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/03-human-review-protocols"},{"id":"01-handbook-core-method/08-evaluation/01-quality-rubric","title":"Quality Rubric","description":"This rubric converts \\"it looks good\\" into a measurable score. Use it to grade AI outputs objectively before acceptance.","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/01-quality-rubric"},{"id":"01-handbook-core-method/08-evaluation/05-scenario-scorecards","title":"Scenario Scorecards","description":"A Scenario Scorecard evaluates the entire lifecycle of an AI-assisted task. It answers: \\"Did the GenAI & LLM Documentation process actually work, or did we just generate code fast?\\"","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/05-scenario-scorecards"},{"id":"06-templates/scenario-scorecard-template","title":"Template: Scenario Scorecard","description":"Use this scorecard to grade an end-to-end AI session. Be honest. If you skipped a step, mark it as skipped.","permalink":"/gen-ai-llm-docs/docs/06-templates/scenario-scorecard-template"}],"unlisted":false}}')}}]);