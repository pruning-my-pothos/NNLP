"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[4349],{28453(e,n,t){t.d(n,{R:()=>s,x:()=>a});var o=t(96540);const i={},r=o.createContext(i);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:n},e.children)}},50435(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"foundations/04-sequential-models/encoder-decoder-model","title":"Encoder-Decoder Model","description":"Understand the powerful Encoder-Decoder model architecture, a foundational component for sequence-to-sequence tasks like machine translation and text summarization, which paved the way for modern Large Language Models (LLMs).","source":"@site/../docs/foundations/04-sequential-models/encoder-decoder-model.md","sourceDirName":"foundations/04-sequential-models","slug":"/foundations/04-sequential-models/encoder-decoder-model","permalink":"/gen-ai-llm-docs/docs/foundations/04-sequential-models/encoder-decoder-model","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":null,"sidebarPosition":5,"frontMatter":{"title":"Encoder-Decoder Model","description":"Understand the powerful Encoder-Decoder model architecture, a foundational component for sequence-to-sequence tasks like machine translation and text summarization, which paved the way for modern Large Language Models (LLMs).","sidebar_position":5},"sidebar":"mainSidebar","previous":{"title":"Types of RNNs: LSTMs and GRUs","permalink":"/gen-ai-llm-docs/docs/foundations/04-sequential-models/types-of-rnns"},"next":{"title":"Beam Search and BLEU Evaluation Metrics","permalink":"/gen-ai-llm-docs/docs/foundations/04-sequential-models/beam-search-and-bleu-evaluation-matrices"}}');var i=t(74848),r=t(28453);const s={title:"Encoder-Decoder Model",description:"Understand the powerful Encoder-Decoder model architecture, a foundational component for sequence-to-sequence tasks like machine translation and text summarization, which paved the way for modern Large Language Models (LLMs).",sidebar_position:5},a="Encoder-Decoder Model",d={},c=[{value:"What is a Sequence-to-Sequence (Seq2Seq) Task?",id:"what-is-a-sequence-to-sequence-seq2seq-task",level:2},{value:"Architecture of the Encoder-Decoder Model",id:"architecture-of-the-encoder-decoder-model",level:2},{value:"1. The Encoder",id:"1-the-encoder",level:3},{value:"2. The Decoder",id:"2-the-decoder",level:3},{value:"Visual Suggestion: Encoder-Decoder Diagram",id:"visual-suggestion-encoder-decoder-diagram",level:2},{value:"Training the Encoder-Decoder Model",id:"training-the-encoder-decoder-model",level:2},{value:"Limitations and the Rise of Attention",id:"limitations-and-the-rise-of-attention",level:2},{value:"Relevance to Generative AI and LLMs",id:"relevance-to-generative-ai-and-llms",level:2},{value:"Next Steps",id:"next-steps",level:2}];function l(e){const n={a:"a",admonition:"admonition",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"encoder-decoder-model",children:"Encoder-Decoder Model"})}),"\n",(0,i.jsxs)(n.admonition,{title:"Analogy: The Human Translator",type:"info",children:[(0,i.jsx)(n.p,{children:"The Encoder-Decoder model works much like a human translator."}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"The Encoder"}),' is the translator listening to an entire sentence in one language, building a complete mental picture of its meaning. This mental picture is the "context vector."']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"The Decoder"})," is the translator then taking that mental picture and speaking a new sentence in the target language, word by word.\nThe key idea is this two-step process: first understand, then generate."]}),"\n"]})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Encoder-Decoder model"})," is a powerful and widely adopted neural network architecture designed for ",(0,i.jsx)(n.strong,{children:"sequence-to-sequence (Seq2Seq)"})," tasks. These are tasks where the input is a sequence, and the output is also a sequence, but often of a different length or type. This model architecture was a significant breakthrough for applications like machine translation and text summarization, directly influencing the development of the Transformer architecture used in modern LLMs."]}),"\n",(0,i.jsx)(n.h2,{id:"what-is-a-sequence-to-sequence-seq2seq-task",children:"What is a Sequence-to-Sequence (Seq2Seq) Task?"}),"\n",(0,i.jsx)(n.p,{children:"A Seq2Seq task involves mapping an input sequence $X = (x_1, x_2, ..., x_n)$ to an output sequence $Y = (y_1, y_2, ..., y_m)$, where $n$ and $m$ can be different."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Examples"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Machine Translation"}),": English sentence $\\rightarrow$ French sentence ($n \\neq m$)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Text Summarization"}),": Long document $\\rightarrow$ Short summary ($n > m$)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Question Answering"}),": Question + Context $\\rightarrow$ Answer"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture-of-the-encoder-decoder-model",children:"Architecture of the Encoder-Decoder Model"}),"\n",(0,i.jsx)(n.p,{children:"The Encoder-Decoder model, at its core, consists of two main RNNs (often LSTMs or GRUs) that work in tandem:"}),"\n",(0,i.jsx)(n.h3,{id:"1-the-encoder",children:"1. The Encoder"}),"\n",(0,i.jsx)(n.admonition,{title:"The Encoder's Job: Create a Summary",type:"tip",children:(0,i.jsxs)(n.p,{children:["The entire job of the encoder is to read the input sequence and create a single, dense numerical summary of it. This summary, the ",(0,i.jsx)(n.strong,{children:"context vector"}),', is expected to capture the complete "meaning" or "intent" of the input sequence.']})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Role"}),": Reads the entire input sequence $X$ word by word (or token by token) and compresses all the information into a fixed-size representation called the ",(0,i.jsx)(n.strong,{children:"context vector"})," (or thought vector, or latent representation)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process"}),": It iterates through the input sequence, updating its hidden state at each time step. The final hidden state of the encoder is typically taken as the context vector."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitation"}),': For long input sequences, compressing all information into a single fixed-size context vector can lead to information loss (the "bottleneck" problem).']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-the-decoder",children:"2. The Decoder"}),"\n",(0,i.jsx)(n.admonition,{title:"The Decoder's Job: Write from the Summary",type:"tip",children:(0,i.jsxs)(n.p,{children:["Following the analogy, the decoder's job is to act as a writer who is given ",(0,i.jsx)(n.em,{children:"only"})," the encoder's summary (the context vector) and nothing else. From that single summary, it must generate a brand new, coherent output sequence. This highlights how much pressure is put on the quality of the context vector."]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Role"}),": Takes the context vector from the encoder as its initial hidden state and generates the output sequence $Y$ one element at a time."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process"}),': At each time step, the decoder receives the previous hidden state, the context vector, and the previously generated output word (or a special "start-of-sequence" token for the first step) to predict the next word in the output sequence. The predicted word is then fed back as an input to the decoder at the next time step.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Conditioning"}),': The decoder is effectively "conditioned" on the input sequence through the context vector.']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"visual-suggestion-encoder-decoder-diagram",children:"Visual Suggestion: Encoder-Decoder Diagram"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TD\n    subgraph Encoder\n        Input1[X1] --\x3e E1(RNN Cell)\n        E1 -- State --\x3e E2(RNN Cell)\n        Input2[X2] --\x3e E2\n        E2 -- State --\x3e E3(RNN Cell)\n        Input3[X3] --\x3e E3\n        E3 -- Final State --\x3e Context(Context Vector)\n    end\n\n    subgraph Decoder\n        Context --\x3e D1(RNN Cell)\n        Start[<SOS>] --\x3e D1\n        D1 --\x3e Output1[Y1]\n        D1 -- State --\x3e D2(RNN Cell)\n        Output1 --\x3e D2\n        D2 --\x3e Output2[Y2]\n        D2 -- State --\x3e D3(RNN Cell)\n        Output2 --\x3e D3\n        D3 --\x3e Output3[Y3]\n        Output3 --\x3e EOS[<EOS>]\n    end\n\n    style Context fill:#ADD8E6,stroke:#87CEEB,color:#000000\n    style E1 fill:#FFE0B2,stroke:#FFCC80,color:#000000\n    style E2 fill:#FFE0B2,stroke:#FFCC80,color:#000000\n    style E3 fill:#FFE0B2,stroke:#FFCC80,color:#000000\n    style D1 fill:#E0FFFF,stroke:#AFEEEE,color:#000000\n    style D2 fill:#E0FFFF,stroke:#AFEEEE,color:#000000\n    style D3 fill:#E0FFFF,stroke:#AFEEEE,color:#000000"}),"\n",(0,i.jsx)(n.h2,{id:"training-the-encoder-decoder-model",children:"Training the Encoder-Decoder Model"}),"\n",(0,i.jsx)(n.p,{children:"The model is trained end-to-end to maximize the probability of the correct output sequence given the input sequence. The training process involves:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Feeding an input sequence to the encoder."}),"\n",(0,i.jsx)(n.li,{children:"Obtaining the context vector."}),"\n",(0,i.jsx)(n.li,{children:"Using the context vector and the actual target output sequence to train the decoder, usually employing teacher forcing (feeding the actual target output of the previous step as input to the current decoder step)."}),"\n",(0,i.jsx)(n.li,{children:"Backpropagating the error through both the decoder and the encoder."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"limitations-and-the-rise-of-attention",children:"Limitations and the Rise of Attention"}),"\n",(0,i.jsx)(n.admonition,{title:"The Bottleneck That Inspired a Revolution",type:"info",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"information bottleneck"}),' of the fixed-size context vector is one of the most important problems in the history of NLP. The question "How can we expect a single vector to summarize a 100-word sentence?" was the direct motivation for the invention of the ',(0,i.jsx)(n.strong,{children:"Attention Mechanism"}),'. Instead of relying on a single summary, attention allowed the decoder to "look back" and focus on different parts of the input sequence at each step of its generation process. This idea completely changed the game and led directly to the Transformer.']})}),"\n",(0,i.jsx)(n.p,{children:"The primary limitation of the vanilla Encoder-Decoder model is the fixed-size context vector. As mentioned, for very long input sequences, it becomes increasingly difficult for this single vector to encapsulate all relevant information, leading to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Information Bottleneck"}),": The context vector becomes a bottleneck, forcing the model to forget earlier parts of the input."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Difficulty with Long Sequences"}),": Performance degrades significantly with longer inputs."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This limitation was elegantly addressed by the introduction of ",(0,i.jsx)(n.strong,{children:"Attention Mechanisms"}),', which allowed the decoder to "look back" at different parts of the input sequence during decoding, providing a more dynamic context vector at each step. This innovation was a crucial precursor to the Transformer architecture.']}),"\n",(0,i.jsx)(n.h2,{id:"relevance-to-generative-ai-and-llms",children:"Relevance to Generative AI and LLMs"}),"\n",(0,i.jsx)(n.p,{children:"The Encoder-Decoder framework established the paradigm for handling complex sequence-to-sequence mappings. It showcased how neural networks could generate novel sequences. The concepts of an encoder extracting features and a decoder generating outputs are fundamental. While modern LLMs often use decoder-only architectures (especially for pure generation tasks), the underlying principles of sequence processing and contextual representation established by the Encoder-Decoder model (and then enhanced by attention) are core to their functionality."}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.p,{children:["The bottleneck of the fixed-size context vector led directly to the development of the ",(0,i.jsx)(n.a,{href:"/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism",children:"Attention Mechanism"}),", a groundbreaking innovation we will explore next."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);