"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[829],{28453(e,n,i){i.d(n,{R:()=>d,x:()=>a});var t=i(96540);const s={},o=t.createContext(s);function d(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),t.createElement(o.Provider,{value:n},e.children)}},63057(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>m,frontMatter:()=>d,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"foundations/03-nlp-basics/embedding-classification-demo","title":"Embedding Classification Demo","description":"A practical demonstration of how word embeddings can be used for text classification, showcasing their power in capturing semantic meaning for machine learning tasks.","source":"@site/../docs/foundations/03-nlp-basics/embedding-classification-demo.md","sourceDirName":"foundations/03-nlp-basics","slug":"/foundations/03-nlp-basics/embedding-classification-demo","permalink":"/gen-ai-llm-docs/docs/foundations/03-nlp-basics/embedding-classification-demo","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":null,"sidebarPosition":7,"frontMatter":{"title":"Embedding Classification Demo","description":"A practical demonstration of how word embeddings can be used for text classification, showcasing their power in capturing semantic meaning for machine learning tasks.","sidebar_position":7},"sidebar":"mainSidebar","previous":{"title":"Embedding Matrix","permalink":"/gen-ai-llm-docs/docs/foundations/03-nlp-basics/embedding-matrix"},"next":{"title":"Introduction to Sequential Data and RNNs","permalink":"/gen-ai-llm-docs/docs/foundations/04-sequential-models/introduction-to-sequential-data-and-rnn"}}');var s=i(74848),o=i(28453);const d={title:"Embedding Classification Demo",description:"A practical demonstration of how word embeddings can be used for text classification, showcasing their power in capturing semantic meaning for machine learning tasks.",sidebar_position:7},a="Embedding Classification Demo",r={},l=[{value:"Concept Overview",id:"concept-overview",level:2},{value:"Actionable Insight: Beyond Keywords",id:"actionable-insight-beyond-keywords",level:2},{value:"Demo: Simple Sentiment Classification",id:"demo-simple-sentiment-classification",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Load Embeddings",id:"step-1-load-embeddings",level:3},{value:"Step 2: Represent Text as Document Embeddings",id:"step-2-represent-text-as-document-embeddings",level:3},{value:"Step 3: Train a Classifier",id:"step-3-train-a-classifier",level:3},{value:"Step 4: Make a Prediction on New Text",id:"step-4-make-a-prediction-on-new-text",level:3},{value:"Visual Suggestion: Embedding Space with Decision Boundary",id:"visual-suggestion-embedding-space-with-decision-boundary",level:2},{value:"Limitations of this Simple Demo",id:"limitations-of-this-simple-demo",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"embedding-classification-demo",children:"Embedding Classification Demo"})}),"\n",(0,s.jsx)(n.admonition,{title:"The Core Principle of Text Classification",type:"info",children:(0,s.jsxs)(n.p,{children:["This demo, while simplified, illustrates one of the most important concepts in NLP: ",(0,s.jsx)(n.strong,{children:"turning words into numbers so you can do math on them"}),". The core idea of converting text to meaningful vectors and then using a classifier is the same principle that powers sentiment analysis, spam detection, and even more complex tasks within Large Language Models."]})}),"\n",(0,s.jsxs)(n.p,{children:["This demo illustrates a fundamental application of word embeddings: ",(0,s.jsx)(n.strong,{children:"Text Classification"}),". By representing words and documents as numerical vectors, we can leverage standard machine learning algorithms to categorize text based on its semantic content. This forms the basis for many NLP applications, from spam detection to sentiment analysis."]}),"\n",(0,s.jsx)(n.h2,{id:"concept-overview",children:"Concept Overview"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text to Embeddings"}),": Raw text (sentences, documents) is converted into numerical vectors using pre-trained word embeddings (e.g., Word2Vec, GloVe) or contextual embeddings (from models like BERT). For simplicity, this demo will average word embeddings to represent a sentence or document."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Representation"}),': Each text document is now a single numerical vector (the "document embedding").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Classification Model"}),": A standard machine learning classifier (e.g., Logistic Regression, Support Vector Machine, or a simple Neural Network) is trained on these document embeddings and their corresponding labels."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prediction"}),": For new, unseen text, its embedding is generated and fed to the trained classifier to predict its category."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"actionable-insight-beyond-keywords",children:"Actionable Insight: Beyond Keywords"}),"\n",(0,s.jsxs)(n.admonition,{title:"Understanding vs. Matching",type:"tip",children:[(0,s.jsx)(n.p,{children:"This is the critical difference between modern and traditional NLP."}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Keyword matching"}),' finds documents that contain the word "happy."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding-based methods"})," find documents that ",(0,s.jsx)(n.em,{children:"are about"}),' happiness, even if they use words like "joyful," "ecstatic," or "content."\nThis shift from matching to understanding is what makes LLMs so powerful.']}),"\n"]})]}),"\n",(0,s.jsxs)(n.p,{children:["Traditional text classification often relies on keyword matching or TF-IDF. Embedding-based classification goes a step further by understanding ",(0,s.jsx)(n.em,{children:"meaning"}),'. For instance, a document containing "joy" and "happiness" will be classified similarly to one with "elation" and "delight," even if they don\'t share exact words.']}),"\n",(0,s.jsx)(n.h2,{id:"demo-simple-sentiment-classification",children:"Demo: Simple Sentiment Classification"}),"\n",(0,s.jsxs)(n.admonition,{title:"Heads-Up: Simplified Demo & File Download",type:"warning",children:[(0,s.jsxs)(n.p,{children:["The following code is a ",(0,s.jsx)(n.strong,{children:"conceptual demonstration"}),", not a production-ready solution. It uses a tiny dataset and a very simple method (averaging embeddings) to illustrate the core idea. We discuss these limitations at the end."]}),(0,s.jsxs)(n.p,{children:["To run this code, you will need to ",(0,s.jsxs)(n.strong,{children:["download the ",(0,s.jsx)(n.code,{children:"glove.6B.50d.txt"})," file"]})," (approx. 175 MB) from the Stanford NLP website."]})]}),"\n",(0,s.jsx)(n.p,{children:"Let's build a conceptual Python demo for classifying movie review sentiment (positive/negative) using pre-trained GloVe embeddings and a Logistic Regression classifier."}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python 3.x"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"numpy"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"scikit-learn"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"nltk"})," (for tokenization, optional)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-trained GloVe embeddings"}),": Download ",(0,s.jsx)(n.code,{children:"glove.6B.50d.txt"})," (50-dimensional embeddings trained on 6 Billion words from Wikipedia + Gigaword 5) from ",(0,s.jsx)(n.a,{href:"https://nlp.stanford.edu/projects/glove/",children:"Stanford NLP GloVe page"}),". Place it in a ",(0,s.jsx)(n.code,{children:"data/"})," directory."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-load-embeddings",children:"Step 1: Load Embeddings"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\ndef load_glove_embeddings(glove_file):\n    \"\"\"Loads GloVe embeddings from a file.\"\"\"\n    word_to_vec = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            word_to_vec[word] = vector\n    return word_to_vec\n\n# Path to your downloaded GloVe file\nGLOVE_FILE = 'data/glove.6B.50d.txt'\nword_vectors = load_glove_embeddings(GLOVE_FILE)\nembedding_dim = len(next(iter(word_vectors.values())))\nprint(f\"Loaded {len(word_vectors)} word vectors of dimension {embedding_dim}.\")\n# Expected Output: Loaded XXXXXX word vectors of dimension 50.\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-represent-text-as-document-embeddings",children:"Step 2: Represent Text as Document Embeddings"}),"\n",(0,s.jsx)(n.p,{children:"We'll average the word embeddings of all words in a sentence/document."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import re\nfrom nltk.tokenize import word_tokenize # pip install nltk; nltk.download(\'punkt\')\n\ndef get_document_embedding(text, word_vectors, embedding_dim):\n    """Generates an average embedding for a document."""\n    tokens = word_tokenize(text.lower())\n    # Remove non-alphabetic tokens and handle out-of-vocabulary words\n    clean_tokens = [token for token in tokens if token.isalpha() and token in word_vectors]\n    \n    if not clean_tokens:\n        return np.zeros(embedding_dim) # Return zero vector if no valid words\n    \n    doc_embedding = np.mean([word_vectors[word] for word in clean_tokens], axis=0)\n    return doc_embedding\n\n# Sample movie review data\nreviews = [\n    ("This movie was fantastic and truly captivating!", "positive"),\n    ("Absolutely terrible, a complete waste of time.", "negative"),\n    ("The plot was okay, but the acting was poor.", "negative"),\n    ("A masterpiece of cinema, highly recommend!", "positive"),\n    ("It was an average film, nothing special.", "negative"), # Misleading label to show limits\n]\n\nX = [] # Document embeddings\ny = [] # Labels (0 for negative, 1 for positive)\n\nfor review_text, sentiment_label in reviews:\n    doc_embed = get_document_embedding(review_text, word_vectors, embedding_dim)\n    X.append(doc_embed)\n    y.append(1 if sentiment_label == "positive" else 0)\n\nX = np.array(X)\ny = np.array(y)\n\nprint(f"Generated {len(X)} document embeddings.")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-train-a-classifier",children:"Step 3: Train a Classifier"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split data (for a real project, much more data is needed)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a simple Logistic Regression classifier\nclassifier = LogisticRegression(max_iter=200) # Increased max_iter for convergence\nclassifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = classifier.predict(X_test)\n\n# Evaluate\nprint("\\nAccuracy:", accuracy_score(y_test, y_pred))\nprint("\\nClassification Report:\\n", classification_report(y_test, y_pred))\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-make-a-prediction-on-new-text",children:"Step 4: Make a Prediction on New Text"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'new_review = "I found the movie quite engaging and well-produced, definitely worth watching."\nnew_review_embed = get_document_embedding(new_review, word_vectors, embedding_dim).reshape(1, -1)\n\nprediction = classifier.predict(new_review_embed)\nsentiment = "positive" if prediction[0] == 1 else "negative"\n\nprint(f"\\nNew review: \'{new_review}\'")\nprint(f"Predicted sentiment: {sentiment}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"visual-suggestion-embedding-space-with-decision-boundary",children:"Visual Suggestion: Embedding Space with Decision Boundary"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    subgraph 2D Embedding Space\n        PointA[Doc A - Positive]:::positive\n        PointB[Doc B - Positive]:::positive\n        PointC[Doc C - Negative]:::negative\n        PointD[Doc D - Negative]:::negative\n        DecisionBoundary(Decision Boundary)\n        PointA --- DecisionBoundary\n        PointB --- DecisionBoundary\n        PointC --- DecisionBoundary\n        PointD --- DecisionBoundary\n    end\n\n    classDef positive fill:#90EE90,stroke:#3CB371,color:#0F1F2E\n    classDef negative fill:#FFCCCC,stroke:#FF0000,color:#0F1F2E"}),"\n",(0,s.jsx)(n.p,{children:"This visual would powerfully show how the classifier uses the spatial arrangement of embeddings to make decisions."}),"\n",(0,s.jsx)(n.h2,{id:"limitations-of-this-simple-demo",children:"Limitations of this Simple Demo"}),"\n",(0,s.jsxs)(n.admonition,{title:"From Limitations to Innovation",type:"info",children:[(0,s.jsxs)(n.p,{children:["The limitations listed here are not just flaws in the demo; they are the very ",(0,s.jsx)(n.strong,{children:"reasons that more advanced models were invented"}),"."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Losing word order by averaging embeddings led to the development of ",(0,s.jsx)(n.strong,{children:"RNNs"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The fixed, context-independent nature of GloVe embeddings led to the creation of contextual models like ",(0,s.jsx)(n.strong,{children:"BERT"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The need to process sequences more efficiently led to the ",(0,s.jsx)(n.strong,{children:"Transformer architecture"}),".\nUnderstanding these limitations helps you appreciate why the models we use today are designed the way they are."]}),"\n"]})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Averaging Embeddings"}),": Simple averaging loses word order and specific contextual nuances. More advanced methods (like LSTMs, GRUs, or Transformers) are needed for richer representations."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Small Dataset"}),": Real-world classification requires much larger, balanced, and diverse datasets."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-trained Embeddings"}),": GloVe is context-independent. Modern LLMs use contextual embeddings (e.g., BERT, GPT) where a word's vector changes based on its surrounding words in a sentence, capturing polysemy."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["This demo provides a hands-on feel for how embeddings power text classification. Next, we will delve into ",(0,s.jsx)(n.a,{href:"/gen-ai-llm-docs/docs/foundations/04-sequential-models/introduction-to-sequential-data-and-rnn",children:"Sequential Data, RNNs, and the Encoder-Decoder Model"}),", which represent an evolution in handling ordered linguistic information more effectively."]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);