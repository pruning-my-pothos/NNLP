"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[2397],{57765(e){e.exports=JSON.parse('{"tag":{"label":"scenarios","permalink":"/gen-ai-llm-docs/docs/tags/scenarios","allTagsPath":"/gen-ai-llm-docs/docs/tags","count":3,"items":[{"id":"03-professional-scenarios/advanced-scenarios/00-index","title":"Advanced Scenarios Index","description":"Theory is clean. Reality is messy. These scenarios are \\"game tapes\\" of real GenAI & LLM execution, showing the messy middle where trade-offs happen, and how to apply the framework to overcome challenges.","permalink":"/gen-ai-llm-docs/docs/03-professional-scenarios/advanced-scenarios/00-index"},{"id":"03-professional-scenarios/00-scenarios-index","title":"Professional Scenarios Index","description":"Theory is clean. Reality is messy. These scenarios are \\"game tapes\\" of real GenAI & LLM Documentation execution, showing the messy middle where trade-offs happen, and how to apply the framework to overcome challenges.","permalink":"/gen-ai-llm-docs/docs/03-professional-scenarios/00-scenarios-index"},{"id":"01-handbook-core-method/08-evaluation/05-scenario-scorecards","title":"Scenario Scorecards","description":"A Scenario Scorecard evaluates the entire lifecycle of an AI-assisted task. It answers: \\"Did the GenAI & LLM Documentation process actually work, or did we just generate code fast?\\"","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/05-scenario-scorecards"}],"unlisted":false}}')}}]);