"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[4382],{28453(e,n,t){t.d(n,{R:()=>l,x:()=>a});var i=t(96540);const s={},o=i.createContext(s);function l(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(o.Provider,{value:n},e.children)}},78457(e,n,t){t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"foundations/02-llm-deep-dive/06-context-windows-and-tokens","title":"Context Windows and Tokens","description":"Help practitioners budget context, avoid truncation, and manage cost/latency trade-offs.","source":"@site/../docs/foundations/02-llm-deep-dive/06-context-windows-and-tokens.md","sourceDirName":"foundations/02-llm-deep-dive","slug":"/foundations/02-llm-deep-dive/06-context-windows-and-tokens","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/06-context-windows-and-tokens","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"context-window","permalink":"/gen-ai-llm-docs/docs/tags/context-window"},{"inline":true,"label":"tokens","permalink":"/gen-ai-llm-docs/docs/tags/tokens"},{"inline":true,"label":"cost","permalink":"/gen-ai-llm-docs/docs/tags/cost"},{"inline":true,"label":"latency","permalink":"/gen-ai-llm-docs/docs/tags/latency"}],"version":"current","lastUpdatedAt":null,"frontMatter":{"title":"Context Windows and Tokens","archetype":"foundation","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["context-window","tokens","cost","latency"],"last_reviewed":"2025-12-20"},"sidebar":"mainSidebar","previous":{"title":"What Are LLMs?","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/01-what-are-llms"},"next":{"title":"Copilots","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/copilots"}}');var s=t(74848),o=t(28453);const l={title:"Context Windows and Tokens",archetype:"foundation",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["context-window","tokens","cost","latency"],last_reviewed:"2025-12-20"},a="Context Windows and Token Economics",r={},d=[{value:"Key Points",id:"key-points",level:2},{value:"Practical Guidance",id:"practical-guidance",level:2},{value:"Anti-Patterns",id:"anti-patterns",level:2}];function c(e){const n={admonition:"admonition",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"context-windows-and-token-economics",children:"Context Windows and Token Economics"})}),"\n",(0,s.jsx)(n.admonition,{title:"Purpose",type:"info",children:(0,s.jsx)(n.p,{children:"Help practitioners budget context, avoid truncation, and manage cost/latency trade-offs."})}),"\n",(0,s.jsx)(n.h2,{id:"key-points",children:"Key Points"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context is finite"}),": prompts + history + retrieved text must fit; older tokens may be dropped or summarized."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tokens drive cost and latency"}),": input/output tokens both bill; longer outputs are slower and costlier."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Position effects"}),": important instructions should be early and near the model\u2019s attention focus."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compression vs recall"}),": summarization saves tokens but risks losing detail; use retrieval for specificity."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-guidance",children:"Practical Guidance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Set ",(0,s.jsx)(n.strong,{children:"budgets"}),": max input tokens, max output tokens; enforce truncation rules."]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.strong,{children:"structured context"}),": headers, bulleting, and schemas to make key facts salient."]}),"\n",(0,s.jsxs)(n.li,{children:["Keep ",(0,s.jsx)(n.strong,{children:"system/intent/constraints"})," short and stable; avoid duplicating boilerplate in every turn when possible."]}),"\n",(0,s.jsxs)(n.li,{children:["Consider ",(0,s.jsx)(n.strong,{children:"streaming"})," and ",(0,s.jsx)(n.strong,{children:"short outputs"})," when latency matters."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"anti-patterns",children:"Anti-Patterns"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pasting entire documents; \u201chope\u201d the model will pick what matters."}),"\n",(0,s.jsx)(n.li,{children:"Ignoring growth of conversation history leading to silent truncation."}),"\n",(0,s.jsx)(n.li,{children:"Letting output length drift without limits or evaluation."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);