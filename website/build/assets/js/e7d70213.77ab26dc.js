"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[410],{27580(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"01-handbook-core-method/core-skills/03-systems","title":"Systems","description":"Systems thinking ensures that AI-generated solutions don\'t break when integrated, scaled, or maintained. It\u2019s about understanding context.","source":"@site/../docs/01-handbook-core-method/core-skills/03-systems.md","sourceDirName":"01-handbook-core-method/core-skills","slug":"/01-handbook-core-method/core-skills/03-systems","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/core-skills/03-systems","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"genai-llm","permalink":"/gen-ai-llm-docs/docs/tags/genai-llm"},{"inline":true,"label":"systems-thinking","permalink":"/gen-ai-llm-docs/docs/tags/systems-thinking"},{"inline":true,"label":"architecture","permalink":"/gen-ai-llm-docs/docs/tags/architecture"},{"inline":true,"label":"integration","permalink":"/gen-ai-llm-docs/docs/tags/integration"},{"inline":true,"label":"dependencies","permalink":"/gen-ai-llm-docs/docs/tags/dependencies"}],"version":"current","lastUpdatedAt":null,"frontMatter":{"title":"Systems","archetype":"core-skill","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["genai-llm","systems-thinking","architecture","integration","dependencies"],"last_reviewed":"2025-12-20"},"sidebar":"mainSidebar","previous":{"title":"Language","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/core-skills/02-language"},"next":{"title":"Sentences","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/core-skills/04-sentences"}}');var t=i(74848),r=i(28453);const l={title:"Systems",archetype:"core-skill",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["genai-llm","systems-thinking","architecture","integration","dependencies"],last_reviewed:"2025-12-20"},a="Systems",d={},c=[{value:"At a Glance",id:"at-a-glance",level:2},{value:"Overview",id:"overview",level:2},{value:"What \u201cSystems\u201d Means in GenAI &amp; LLM Documentation",id:"what-systems-means-in-genai--llm-documentation",level:2},{value:"Why Systems Discipline Matters with AI",id:"why-systems-discipline-matters-with-ai",level:2},{value:"Common System Concepts in GenAI &amp; LLM Documentation",id:"common-system-concepts-in-genai--llm-documentation",level:2},{value:"Boundaries and Interfaces",id:"boundaries-and-interfaces",level:3},{value:"State Management",id:"state-management",level:3},{value:"Error Handling and Fallbacks",id:"error-handling-and-fallbacks",level:3},{value:"Scalability and Performance",id:"scalability-and-performance",level:3},{value:"Security Considerations",id:"security-considerations",level:3},{value:"Observability",id:"observability",level:3},{value:"Systems-Level Failure Modes",id:"systems-level-failure-modes",level:2},{value:"Isolated Solutions",id:"isolated-solutions",level:3},{value:"Unintended Side Effects",id:"unintended-side-effects",level:3},{value:"Performance Degradation",id:"performance-degradation",level:3},{value:"Security Vulnerabilities",id:"security-vulnerabilities",level:3},{value:"Systems Across the GenAI &amp; LLM Documentation Method",id:"systems-across-the-genai--llm-documentation-method",level:2},{value:"Practical Systems Checklist",id:"practical-systems-checklist",level:2},{value:"Systems and Review",id:"systems-and-review",level:2},{value:"How to Practice Systems Thinking in GenAI &amp; LLM Documentation",id:"how-to-practice-systems-thinking-in-genai--llm-documentation",level:2},{value:"Last Reviewed / Last Updated",id:"last-reviewed--last-updated",level:2}];function o(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"systems",children:"Systems"})}),"\n",(0,t.jsx)(n.admonition,{title:"Core idea",type:"info",children:(0,t.jsx)(n.p,{children:"Systems thinking ensures that AI-generated solutions don't break when integrated, scaled, or maintained. It\u2019s about understanding context."})}),"\n",(0,t.jsx)(n.h2,{id:"at-a-glance",children:"At a Glance"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Focus"}),(0,t.jsx)(n.th,{children:"Use Systems Thinking To"}),(0,t.jsx)(n.th,{children:"Failure If Missing"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Dependencies & interfaces"}),(0,t.jsx)(n.td,{children:"Define explicit contracts"}),(0,t.jsx)(n.td,{children:"Solutions break downstream"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"State & lifecycle"}),(0,t.jsx)(n.td,{children:"Manage data flow and mutations"}),(0,t.jsx)(n.td,{children:"Data corruption, race conditions"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Failure modes & recovery"}),(0,t.jsx)(n.td,{children:"Anticipate and plan for errors"}),(0,t.jsx)(n.td,{children:"Cascading failures"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Context & environment"}),(0,t.jsx)(n.td,{children:"Understand where solution will operate"}),(0,t.jsx)(n.td,{children:"Deploying incompatible solutions"})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["In GenAI & LLM Documentation, systems thinking ensures that language-driven execution does not produce isolated solutions that break when integrated, scaled, or maintained. It\u2019s the skill of ",(0,t.jsx)(n.strong,{children:"understanding the broader context in which your AI-assisted work operates"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"AI is good at generating localized solutions. Your job is to make sure those solutions fit into the larger system without causing unintended consequences."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"what-systems-means-in-genai--llm-documentation",children:"What \u201cSystems\u201d Means in GenAI & LLM Documentation"}),"\n",(0,t.jsxs)(n.p,{children:["Systems in GenAI & LLM Documentation refers to understanding ",(0,t.jsx)(n.strong,{children:"how components interact over time"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"It means:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"identifying all upstream and downstream dependencies"}),"\n",(0,t.jsx)(n.li,{children:"mapping data flow and state changes"}),"\n",(0,t.jsx)(n.li,{children:"considering performance, security, and scalability"}),"\n",(0,t.jsx)(n.li,{children:"anticipating failure modes and designing for resilience"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"GenAI & LLM Documentation treats systems thinking as a safety mechanism."}),"\n",(0,t.jsx)(n.admonition,{title:"Signal of quality",type:"tip",children:(0,t.jsx)(n.p,{children:"If you can draw an architectural diagram of the system where your AI-generated code will live, your systems thinking is strong."})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"why-systems-discipline-matters-with-ai",children:"Why Systems Discipline Matters with AI"}),"\n",(0,t.jsx)(n.p,{children:"LLMs:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"have no inherent understanding of system architecture"}),"\n",(0,t.jsx)(n.li,{children:"treat code as independent text, not interconnected components"}),"\n",(0,t.jsx)(n.li,{children:'will generate "perfect" local solutions that break the larger system'}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"If systems context is missing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"AI will ignore side effects or critical dependencies"}),"\n",(0,t.jsx)(n.li,{children:"generated code will cause integration nightmares"}),"\n",(0,t.jsx)(n.li,{children:"performance bottlenecks will be introduced silently"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"GenAI & LLM Documentation treats systems thinking as a safety mechanism."}),"\n",(0,t.jsx)(n.admonition,{title:"Stop here",type:"danger",children:(0,t.jsx)(n.p,{children:'If your prompt starts with "Generate an isolated microservice...", but you don\'t define how it talks to existing services, your systems thinking is incomplete.'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"common-system-concepts-in-genai--llm-documentation",children:"Common System Concepts in GenAI & LLM Documentation"}),"\n",(0,t.jsx)(n.h3,{id:"boundaries-and-interfaces",children:"Boundaries and Interfaces"}),"\n",(0,t.jsx)(n.p,{children:"Define clear separation between components and how they communicate."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:['"The new ',(0,t.jsx)(n.code,{children:"UserService"})," must only interact with the database through the ",(0,t.jsx)(n.code,{children:"UserRepository"}),' interface."']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"state-management",children:"State Management"}),"\n",(0,t.jsx)(n.p,{children:"How data is created, read, updated, and deleted across the system."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"User session data should be stored in a distributed cache, not in local memory."'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"error-handling-and-fallbacks",children:"Error Handling and Fallbacks"}),"\n",(0,t.jsx)(n.p,{children:"How the system behaves under failure conditions."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\"If the external payment gateway fails, log the error, send a notification, and revert the order status to 'pending'.\""}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scalability-and-performance",children:"Scalability and Performance"}),"\n",(0,t.jsx)(n.p,{children:"How the solution performs under load."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"The API endpoint must handle 1000 requests per second with a p99 latency of < 50ms."'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,t.jsx)(n.p,{children:"Protection against vulnerabilities."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"All user input must be sanitized to prevent XSS and SQL injection."'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"observability",children:"Observability"}),"\n",(0,t.jsx)(n.p,{children:"How the system's internal state can be inferred from its external outputs (logs, metrics, traces)."}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Ensure all critical operations emit structured logs to our ELK stack."'}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"systems-level-failure-modes",children:"Systems-Level Failure Modes"}),"\n",(0,t.jsx)(n.h3,{id:"isolated-solutions",children:"Isolated Solutions"}),"\n",(0,t.jsx)(n.p,{children:"Solving a problem in a vacuum, ignoring broader impact."}),"\n",(0,t.jsx)(n.p,{children:"Effect:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"integration failures"}),"\n",(0,t.jsx)(n.li,{children:"broken dependencies"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Fix:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"explicitly map system context"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"unintended-side-effects",children:"Unintended Side Effects"}),"\n",(0,t.jsx)(n.p,{children:"Changes in one area cause unexpected behavior elsewhere."}),"\n",(0,t.jsx)(n.p,{children:"Effect:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"regressions"}),"\n",(0,t.jsx)(n.li,{children:"debugging nightmares"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Fix:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"provide comprehensive context on dependencies"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"performance-degradation",children:"Performance Degradation"}),"\n",(0,t.jsx)(n.p,{children:"Introducing inefficient logic or resource-intensive operations."}),"\n",(0,t.jsx)(n.p,{children:"Effect:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"slow response times"}),"\n",(0,t.jsx)(n.li,{children:"increased infrastructure costs"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Fix:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"define performance constraints explicitly"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"security-vulnerabilities",children:"Security Vulnerabilities"}),"\n",(0,t.jsx)(n.p,{children:"Generating insecure code or configurations."}),"\n",(0,t.jsx)(n.p,{children:"Effect:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"data breaches"}),"\n",(0,t.jsx)(n.li,{children:"system compromise"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Fix:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"specify security constraints and best practices"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"systems-across-the-genai--llm-documentation-method",children:"Systems Across the GenAI & LLM Documentation Method"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Discovery Brief"}),"\nIdentify system boundaries and stakeholders affected"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Intent Spec"}),"\nDefine success criteria in terms of system behavior"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Constraint Spec"}),"\nEncode system-level requirements (performance, security, resilience)"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Delegation Contract"}),"\nSpecify which parts of the system the AI can interact with"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Generation Requests"}),"\nProvide architectural context for generated code"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Review"}),"\nVerify that the solution integrates correctly and doesn't introduce regressions"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance"}),"\nConfirm the solution meets system-wide quality attributes"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Systems thinking integrates individual components into a cohesive whole."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"practical-systems-checklist",children:"Practical Systems Checklist"}),"\n",(0,t.jsx)(n.p,{children:"Before generating:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"What are the direct dependencies of this change?"}),"\n",(0,t.jsx)(n.li,{children:"What are the indirect dependencies (downstream systems)?"}),"\n",(0,t.jsx)(n.li,{children:"How does data flow through this component?"}),"\n",(0,t.jsx)(n.li,{children:"What happens if a dependency fails?"}),"\n",(0,t.jsx)(n.li,{children:"What security implications does this change have?"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"If unsure, specify."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"systems-and-review",children:"Systems and Review"}),"\n",(0,t.jsx)(n.p,{children:"During review:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Analyze the AI's output through the lens of the entire system."}),"\n",(0,t.jsx)(n.li,{children:"Does it respect boundaries? Are interfaces clean?"}),"\n",(0,t.jsx)(n.li,{children:"Are failure modes handled? Are logs sufficient?"}),"\n",(0,t.jsx)(n.li,{children:"If the AI overlooked a system aspect, identify where the input context was incomplete."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"how-to-practice-systems-thinking-in-genai--llm-documentation",children:"How to Practice Systems Thinking in GenAI & LLM Documentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Draw data flow diagrams before coding."}),"\n",(0,t.jsx)(n.li,{children:"List all services/APIs a component interacts with."}),"\n",(0,t.jsx)(n.li,{children:'Explicitly state non-functional requirements (e.g., "must be highly available").'}),"\n",(0,t.jsx)(n.li,{children:"Model failure conditions and recovery steps."}),"\n",(0,t.jsx)(n.li,{children:'Think about the "blast radius" of any change.'}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{title:"Fast drill",type:"tip",children:(0,t.jsx)(n.p,{children:"For a given feature, list all the services, databases, and external APIs it touches. For each, describe its expected input, output, and failure modes."})}),"\n",(0,t.jsx)(n.p,{children:"This will focus on sentence-level precision and why small wording changes matter disproportionately in GenAI & LLM Documentation."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"last-reviewed--last-updated",children:"Last Reviewed / Last Updated"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Last reviewed: 2025-12-20"}),"\n",(0,t.jsx)(n.li,{children:"Version: 0.1.0"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},28453(e,n,i){i.d(n,{R:()=>l,x:()=>a});var s=i(96540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);