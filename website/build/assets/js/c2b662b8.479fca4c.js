"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[8750],{17670(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"foundations/02-llm-deep-dive/fundamentals/01-how-llms-work-enough-for-practice","title":"How LLMs Work (Enough for Practice)","description":"Give practitioners the minimum mental model to avoid misuse, set correct expectations, and build safely with Large Language Models.","source":"@site/../docs/foundations/02-llm-deep-dive/fundamentals/01-how-llms-work-enough-for-practice.md","sourceDirName":"foundations/02-llm-deep-dive/fundamentals","slug":"/foundations/02-llm-deep-dive/fundamentals/01-how-llms-work-enough-for-practice","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/fundamentals/01-how-llms-work-enough-for-practice","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"llm","permalink":"/gen-ai-llm-docs/docs/tags/llm"},{"inline":true,"label":"fundamentals","permalink":"/gen-ai-llm-docs/docs/tags/fundamentals"},{"inline":true,"label":"probabilistic","permalink":"/gen-ai-llm-docs/docs/tags/probabilistic"},{"inline":true,"label":"transformer","permalink":"/gen-ai-llm-docs/docs/tags/transformer"}],"version":"current","lastUpdatedAt":null,"frontMatter":{"title":"How LLMs Work (Enough for Practice)","archetype":"foundation","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["llm","fundamentals","probabilistic","transformer"],"last_reviewed":"2025-12-28"},"sidebar":"mainSidebar","previous":{"title":"GenAI & LLM Fundamentals","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/fundamentals/00-fundamentals-index"},"next":{"title":"Context Windows and Token Economics","permalink":"/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/fundamentals/02-context-windows-and-token-economics"}}');var r=n(74848),o=n(28453);const s={title:"How LLMs Work (Enough for Practice)",archetype:"foundation",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["llm","fundamentals","probabilistic","transformer"],last_reviewed:"2025-12-28"},a="How LLMs Work (Enough for Practice)",l={},c=[{value:"Quick Model: The Probabilistic Predictor",id:"quick-model-the-probabilistic-predictor",level:2},{value:"Visual: Transformer Decoder Predicting Next Token",id:"visual-transformer-decoder-predicting-next-token",level:3},{value:"What to Remember",id:"what-to-remember",level:2},{value:"How to Apply This Knowledge (Actionable Insights)",id:"how-to-apply-this-knowledge-actionable-insights",level:2},{value:"Code Example: Understanding Temperature&#39;s Effect (Conceptual)",id:"code-example-understanding-temperatures-effect-conceptual",level:3},{value:"Anti-Patterns",id:"anti-patterns",level:2}];function d(e){const t={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"how-llms-work-enough-for-practice",children:"How LLMs Work (Enough for Practice)"})}),"\n",(0,r.jsx)(t.admonition,{title:"Purpose",type:"info",children:(0,r.jsx)(t.p,{children:"Give practitioners the minimum mental model to avoid misuse, set correct expectations, and build safely with Large Language Models."})}),"\n",(0,r.jsx)(t.h2,{id:"quick-model-the-probabilistic-predictor",children:"Quick Model: The Probabilistic Predictor"}),"\n",(0,r.jsxs)(t.p,{children:["At their core, Large Language Models (LLMs) are sophisticated ",(0,r.jsx)(t.strong,{children:"next-token prediction engines"}),". They are built primarily on the Transformer architecture, specifically its decoder-only variant for generative tasks."]}),"\n",(0,r.jsx)(t.h3,{id:"visual-transformer-decoder-predicting-next-token",children:"Visual: Transformer Decoder Predicting Next Token"}),"\n",(0,r.jsx)(t.mermaid,{value:'%%{init: { "flowchart": { "useMaxWidth": true, "nodeSpacing": 110, "rankSpacing": 110 }, "themeVariables": { "fontSize": "16px", "fontFamily": "Inter, system-ui, sans-serif", "lineColor": "#94A3B8" } }}%%\nflowchart LR\n    %% Wide horizontal layout with roomy spacing\n    classDef input fill:#F3E8FF,stroke:#5B21B6,stroke-width:2px,color:#111;\n    classDef process fill:#E0F2FE,stroke:#0369A1,stroke-width:2px,color:#0B3B5C;\n    classDef output fill:#DCFCE7,stroke:#15803D,stroke-width:2px,color:#0F2F1A;\n\n    Input["User prompt + past tokens"]:::input\n    Embed["1) Embed tokens"]:::process\n    Attn["2) Transformer blocks<br/>(self-attention + MLP)"]:::process\n    Logits["3) Logits (scores for vocab)"]:::process\n    Prob["4) Softmax (probabilities)"]:::process\n    Sample["5) Sample/select next token"]:::output\n\n    Input --\x3e Embed --\x3e Attn --\x3e Logits --\x3e Prob --\x3e Sample\n    Sample -- "append & repeat" --\x3e Input'}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Transformer Decoder"}),": This architecture efficiently processes input sequences, paying attention to relationships between tokens, to generate a probability distribution over the entire vocabulary for what token should come next."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Training"}),": LLMs undergo a massive self-supervised training phase on vast amounts of text data (e.g., Common Crawl, Wikipedia, books, code). During this, they learn to predict masked tokens or the next token in a sequence. This process allows them to internalize grammar, facts, common sense (statistical patterns), and various writing styles, but it is fundamentally a pattern-fitting exercise, not symbolic reasoning or memorization."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Inference"}),": When you provide a prompt, the model uses its learned patterns to iteratively predict the most probable next tokens. This process involves ",(0,r.jsx)(t.strong,{children:"sampling"})," from the predicted probability distribution. ",(0,r.jsx)(t.strong,{children:"Sampling parameters"})," like ",(0,r.jsx)(t.code,{children:"temperature"})," and ",(0,r.jsx)(t.code,{children:"top-p"})," control the randomness and diversity of the generated output."]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"what-to-remember",children:"What to Remember"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Probabilistic, not authoritative"}),': LLM output is the most statistically likely continuation, not necessarily ground truth. Its "confidence" is a measure of probability, not factual accuracy or reasoning.']}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Context-bound"}),': The model only "sees" the provided context window (input prompt + generated output so far) and its frozen training weights. It has no long-term memory or external knowledge unless explicitly provided.']}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Tokenized"}),': LLMs operate on tokens (subwords, characters), not full words. This affects context length limits, cost, and how the model "perceives" input. Longer inputs cost more and risk truncation.']}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"No agency or understanding"}),': It does not "decide," "think," or "understand" in a human sense. It completes patterns you set; it doesn\'t possess intent or common sense reasoning.']}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"how-to-apply-this-knowledge-actionable-insights",children:"How to Apply This Knowledge (Actionable Insights)"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Explicitly Narrow the Distribution"}),": To get predictable results, your prompt must act as a powerful filter. State intent, constraints, and desired output format explicitly. Use few-shot examples to demonstrate the precise behavior you want."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Curate Context"}),': Only provide relevant context within the model\'s window. Over-long or irrelevant context can dilute instructions or lead to "lost in the middle" phenomena. For external knowledge, use Retrieval Augmented Generation (RAG).']}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Control Stochasticity"}),":","\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["For deterministic tasks (e.g., code generation, factual extraction), set ",(0,r.jsx)(t.code,{children:"temperature"})," to a very low value (often 0 or near 0) to reduce randomness and increase predictability."]}),"\n",(0,r.jsxs)(t.li,{children:["For creative tasks (e.g., brainstorming, story generation), increase ",(0,r.jsx)(t.code,{children:"temperature"})," to allow for more diverse and surprising outputs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Always Verify with Evidence"}),": Treat all LLM output as a draft. Implement validation steps (e.g., running generated code, checking facts against authoritative sources, schema validation) before accepting or deploying."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Token Awareness"}),": Be mindful of context window limits and token costs. Design prompts and systems to be token-efficient, especially for high-volume or latency-sensitive applications."]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"code-example-understanding-temperatures-effect-conceptual",children:"Code Example: Understanding Temperature's Effect (Conceptual)"}),"\n",(0,r.jsx)(t.p,{children:'Temperature is a hyperparameter that directly influences the randomness of the model\'s output. Higher temperature values make the output more random, creative, and "surprising," while lower values make it more deterministic and focused.'}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import random\n\n# Conceptual function to simulate LLM prediction\ndef conceptual_llm_predict(prompt, temperature=0.7):\n    # In a real LLM, this involves logits, softmax, and sampling.\n    # Here, we\'ll simulate by picking from a set of possible continuations\n    # with varying "randomness".\n\n    possible_continuations = {\n        "The capital of France is": ["Paris.", "London.", "Rome."],\n        "Write a poem about a cat": [\n            "A furry friend, so soft and neat, sleeps all day, a purring treat.",\n            "Upon the sill, a feline dream, chasing sunbeams with a gleam.",\n            "Meow, meow, goes the cat, sitting on the welcome mat."\n        ],\n        "List three programming languages": ["Python, Java, C++.", "Ruby, Go, Swift.", "JavaScript, Rust, PHP."]\n    }\n\n    if prompt not in possible_continuations:\n        return "I don\'t know."\n\n    choices = possible_continuations[prompt]\n\n    if temperature < 0.2: # Very low temp -> very deterministic\n        return choices[0] # Always pick the first, most probable\n    elif temperature < 0.8: # Medium temp -> some variation\n        return random.choice(choices)\n    else: # High temp -> more varied, potentially less coherent\n        # Simulate picking from a wider, less related set for high temp\n        all_options = [item for sublist in possible_continuations.values() for item in sublist]\n        return random.choice(all_options)\n\nprint("--- Low Temperature (0.1) ---")\nprint(conceptual_llm_predict("The capital of France is", temperature=0.1))\nprint(conceptual_llm_predict("Write a poem about a cat", temperature=0.1))\nprint(conceptual_llm_predict("List three programming languages", temperature=0.1))\n\nprint("\\n--- Medium Temperature (0.7) ---")\nprint(conceptual_llm_predict("The capital of France is", temperature=0.7))\nprint(conceptual_llm_predict("Write a poem about a cat", temperature=0.7))\nprint(conceptual_llm_predict("List three programming languages", temperature=0.7))\n\nprint("\\n--- High Temperature (1.2) ---")\nprint(conceptual_llm_predict("The capital of France is", temperature=1.2))\nprint(conceptual_llm_predict("Write a poem about a cat", temperature=1.2))\nprint(conceptual_llm_predict("List three programming languages", temperature=1.2))\n'})}),"\n",(0,r.jsx)(t.h2,{id:"anti-patterns",children:"Anti-Patterns"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Assuming model output is factual or authoritative"}),": Blindly trusting generations without independent verification."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Relying on \u201cit said it checked\u201d"}),": The model confirming its own work provides no real guarantee of correctness."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Ignoring context length"}),": Leading to silent truncation of critical instructions or information, resulting in unexpected behavior."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Treating LLMs as traditional software"}),": Forgetting their probabilistic nature and expecting 100% deterministic, bug-free output from a single prompt."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Failing to control stochasticity"}),": Using high temperatures for tasks requiring precision and determinism."]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453(e,t,n){n.d(t,{R:()=>s,x:()=>a});var i=n(96540);const r={},o=i.createContext(r);function s(e){const t=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(o.Provider,{value:t},e.children)}}}]);