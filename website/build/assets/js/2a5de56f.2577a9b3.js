"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[2557],{68416(e){e.exports=JSON.parse('{"tag":{"label":"scorecard","permalink":"/gen-ai-llm-docs/docs/tags/scorecard","allTagsPath":"/gen-ai-llm-docs/docs/tags","count":2,"items":[{"id":"01-handbook-core-method/08-evaluation/05-scenario-scorecards","title":"Scenario Scorecards","description":"A Scenario Scorecard evaluates the entire lifecycle of an AI-assisted task. It answers: \\"Did the GenAI & LLM Documentation process actually work, or did we just generate code fast?\\"","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/05-scenario-scorecards"},{"id":"06-templates/scenario-scorecard-template","title":"Template: Scenario Scorecard","description":"Use this scorecard to grade an end-to-end AI session. Be honest. If you skipped a step, mark it as skipped.","permalink":"/gen-ai-llm-docs/docs/06-templates/scenario-scorecard-template"}],"unlisted":false}}')}}]);