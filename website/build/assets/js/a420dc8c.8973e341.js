"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[8426],{1809(e,t,i){i.r(t),i.d(t,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"04-responsible-ai/threat-model-lite","title":"Threat Model Lite","description":"Quickly identify and mitigate security risks associated with AI-assisted development. This lightweight threat modeling approach focuses on AI-specific vulnerabilities and ensures that security considerations are embedded early in the GenAI & LLM Documentation Loop.","source":"@site/../docs/04-responsible-ai/threat-model-lite.md","sourceDirName":"04-responsible-ai","slug":"/04-responsible-ai/threat-model-lite","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/threat-model-lite","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"genai-llm","permalink":"/gen-ai-llm-docs/docs/tags/genai-llm"},{"inline":true,"label":"security","permalink":"/gen-ai-llm-docs/docs/tags/security"},{"inline":true,"label":"risk","permalink":"/gen-ai-llm-docs/docs/tags/risk"},{"inline":true,"label":"threat-modeling","permalink":"/gen-ai-llm-docs/docs/tags/threat-modeling"},{"inline":true,"label":"method","permalink":"/gen-ai-llm-docs/docs/tags/method"}],"version":"current","lastUpdatedAt":null,"frontMatter":{"title":"Threat Model Lite","archetype":"guardrail","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["genai-llm","security","risk","threat-modeling","method"],"last_reviewed":"2025-12-28"},"sidebar":"mainSidebar","previous":{"title":"Guardrails and Governance Index","permalink":"/gen-ai-llm-docs/docs/04-responsible-ai/guardrails-index"},"next":{"title":"Templates","permalink":"/gen-ai-llm-docs/docs/06-templates/00-templates-index"}}');var s=i(74848),r=i(28453);const l={title:"Threat Model Lite",archetype:"guardrail",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["genai-llm","security","risk","threat-modeling","method"],last_reviewed:"2025-12-28"},a="Threat Model Lite",o={},d=[{value:"Overview",id:"overview",level:2},{value:"When to Use",id:"when-to-use",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"The Pattern (Step-by-Step)",id:"the-pattern-step-by-step",level:2},{value:"Step 1: Identify Assets",id:"step-1-identify-assets",level:3},{value:"Step 2: Identify Attackers",id:"step-2-identify-attackers",level:3},{value:"Step 3: Enumerate Attack Vectors",id:"step-3-enumerate-attack-vectors",level:3},{value:"Step 4: Define Controls &amp; Mitigations",id:"step-4-define-controls--mitigations",level:3},{value:"Step 5: Prioritize &amp; Review",id:"step-5-prioritize--review",level:3},{value:"Practical Example: Threat Modeling a Code Generation Agent",id:"practical-example-threat-modeling-a-code-generation-agent",level:2},{value:"Common Pitfalls",id:"common-pitfalls",level:2}];function c(e){const t={admonition:"admonition",blockquote:"blockquote",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"threat-model-lite",children:"Threat Model Lite"})}),"\n",(0,s.jsx)(t.admonition,{title:"Value Proposition",type:"info",children:(0,s.jsx)(t.p,{children:"Quickly identify and mitigate security risks associated with AI-assisted development. This lightweight threat modeling approach focuses on AI-specific vulnerabilities and ensures that security considerations are embedded early in the GenAI & LLM Documentation Loop."})}),"\n",(0,s.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(t.p,{children:'Traditional threat modeling can be resource-intensive. For AI-assisted development, a "Threat Model Lite" approach allows teams to rapidly assess and address common AI-related security risks without extensive overhead. This guardrail integrates security thinking directly into the GenAI & LLM Documentation workflow, focusing on identifying potential malicious inputs, unintended behaviors, and data exposures.'}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Goal"}),": Identify and prioritize AI-specific security threats and propose mitigations efficiently.\n",(0,s.jsx)(t.strong,{children:"Anti-pattern"}),": Relying solely on general application security practices, which often miss AI-specific attack vectors, or ignoring security until late in the development cycle."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"when-to-use",children:"When to Use"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"\u2705 Use This Pattern When..."}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"\ud83d\udeab Do Not Use When..."})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Delegating code generation to AI"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"You are developing a simple, non-sensitive internal tool without external access"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Handling sensitive data with AI"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"A full, comprehensive threat model is already in progress by a dedicated security team"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Integrating third-party AI APIs"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"The AI functionality is purely passive (e.g., simple text summarization of public data)"})]})]})]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(t.admonition,{title:"Before you start",type:"warning",children:(0,s.jsx)(t.p,{children:"A basic understanding of common application security concepts (e.g., OWASP Top 10) and the principles of data boundaries is essential."})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Artifacts"}),": Intent Spec, Constraint Spec, Delegation Contract."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Context"}),": Understanding of the AI tool's capabilities, data handling, and deployment environment."]}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"the-pattern-step-by-step",children:"The Pattern (Step-by-Step)"}),"\n",(0,s.jsx)(t.h3,{id:"step-1-identify-assets",children:"Step 1: Identify Assets"}),"\n",(0,s.jsx)(t.p,{children:"What are the valuable things (data, systems, user trust) that the AI-assisted component interacts with or could impact?"}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Practical Insight"}),": Focus on assets that, if compromised, would cause significant harm (e.g., production database, user PII, intellectual property)."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"step-2-identify-attackers",children:"Step 2: Identify Attackers"}),"\n",(0,s.jsx)(t.p,{children:"Who would want to harm these assets, and what are their motivations and capabilities? Consider internal and external threats."}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:'"Attackers could be malicious external users, disgruntled employees, or a compromised third-party AI provider."'}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"step-3-enumerate-attack-vectors",children:"Step 3: Enumerate Attack Vectors"}),"\n",(0,s.jsx)(t.p,{children:"How could an attacker exploit the AI-assisted system? Think about prompt injection, data poisoning, model stealing, and denial of service."}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:'"A malicious user could try prompt injection to make the AI generate harmful code or bypass security checks."'}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"step-4-define-controls--mitigations",children:"Step 4: Define Controls & Mitigations"}),"\n",(0,s.jsx)(t.p,{children:"For each identified threat, propose specific actions or controls to reduce the risk. These should feed directly into your Constraint Spec and Delegation Contract."}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:'"Mitigation: Implement strict input sanitization. Use a Delegation Contract to prohibit the AI from modifying authentication logic. Ensure all AI-generated code is reviewed for prompt injection vulnerabilities."'}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"step-5-prioritize--review",children:"Step 5: Prioritize & Review"}),"\n",(0,s.jsx)(t.p,{children:"Prioritize threats based on likelihood and impact. Review the threat model with security stakeholders."}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:'"High priority: Prompt injection leading to data exfiltration. Mitigation: Enhanced input validation and output filtering."'}),"\n"]}),"\n",(0,s.jsx)(t.mermaid,{value:"flowchart LR\n    A[Identify Assets] --\x3e B[Identify Attackers]\n    B --\x3e C[Enumerate Attack Vectors]\n    C --\x3e D[Define Controls/Mitigations]\n    D --\x3e E[Prioritize & Review]\n\n    classDef step fill:#E6F7FF,stroke:#1B75BB,color:#0F1F2E;\n    class A,B,C,D,E step;"}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"practical-example-threat-modeling-a-code-generation-agent",children:"Practical Example: Threat Modeling a Code Generation Agent"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Objective"}),": Threat model an AI agent generating a new API endpoint based on a user prompt."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Threat Model Lite Analysis:"})}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Assets:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Source code repository (integrity, confidentiality)."}),"\n",(0,s.jsx)(t.li,{children:"API endpoint (availability, data integrity, user authentication)."}),"\n",(0,s.jsx)(t.li,{children:"User data (confidentiality, integrity)."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Attackers:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"External malicious actor (e.g., attempting to exploit the generated API)."}),"\n",(0,s.jsx)(t.li,{children:"Internal developer (e.g., accidentally introducing vulnerabilities with an AI-generated change)."}),"\n",(0,s.jsx)(t.li,{children:"Compromised AI model (e.g., fine-tuned on malicious data)."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Attack Vectors:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Prompt Injection"}),": Malicious instructions in the prompt make the AI generate vulnerable code (e.g., SQL injection, XSS)."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Data Leakage"}),": AI accidentally includes sensitive data from its training or context in the generated code/docs."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Supply Chain Attack"}),": AI introduces malicious dependencies or unapproved libraries."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Denial of Service"}),": AI generates inefficient code that degrades API performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Controls & Mitigations:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Prompt Injection"}),": Implement strict input validation on the user's prompt (Constraint Spec). Use Delegation Contract to forbid direct database access. Rigorous human review of generated code."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Data Leakage"}),': Implement Data Boundaries. Prohibit AI from accessing "Red Zone" data. Review all AI outputs for accidental PII/secret exposure.']}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Supply Chain"}),": Delegation Contract to prohibit new dependencies. Automated dependency scanning."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"DoS"}),": Constraint Spec to define performance requirements. Automated performance testing of generated code."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Prioritization"}),":"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Prompt injection and data leakage are high priority due to direct impact on security and data."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"common-pitfalls",children:"Common Pitfalls"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Pitfall"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Impact"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Correction"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Ignoring AI-specific Threats"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Traditional threats models miss prompt injection, data poisoning, etc."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Explicitly consider AI-specific attack vectors."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Over-Abstracting Threats"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Threats remain theoretical, hard to mitigate."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Focus on concrete, actionable attack scenarios."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Treating Threats as Static"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"New vulnerabilities emerge with new AI capabilities."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Regularly revisit and update the threat model as AI tools evolve."})]})]})]}),"\n",(0,s.jsx)(t.admonition,{title:"Critical Risk",type:"danger",children:(0,s.jsx)(t.p,{children:"Never assume your AI is inherently secure. Proactively threat model all AI-assisted components, especially those handling sensitive data or operating in critical paths."})})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},28453(e,t,i){i.d(t,{R:()=>l,x:()=>a});var n=i(96540);const s={},r=n.createContext(s);function l(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);