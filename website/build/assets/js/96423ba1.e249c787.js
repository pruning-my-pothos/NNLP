"use strict";(self.webpackChunkgenai_llm_documentation_website=self.webpackChunkgenai_llm_documentation_website||[]).push([[2285],{40917(e){e.exports=JSON.parse('{"tag":{"label":"metrics","permalink":"/gen-ai-llm-docs/docs/tags/metrics","allTagsPath":"/gen-ai-llm-docs/docs/tags","count":4,"items":[{"id":"01-handbook-core-method/08-evaluation/00-eval-overview","title":"Evaluation Overview","description":"Evaluation shifts \\"looks good\\" to \\"is good,\\" providing objective evidence that AI-assisted work meets quality and performance criteria.","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/00-eval-overview"},{"id":"01-handbook-core-method/08-evaluation/04-metrics-and-kpis","title":"Metrics and KPIs","description":"How do you know if GenAI & LLM Documentation is working? These metrics help you measure the quality and efficiency of your AI adoption, moving beyond \\"it feels faster.\\"","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/04-metrics-and-kpis"},{"id":"01-handbook-core-method/08-evaluation/01-quality-rubric","title":"Quality Rubric","description":"This rubric converts \\"it looks good\\" into a measurable score. Use it to grade AI outputs objectively before acceptance.","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/01-quality-rubric"},{"id":"01-handbook-core-method/08-evaluation/05-scenario-scorecards","title":"Scenario Scorecards","description":"A Scenario Scorecard evaluates the entire lifecycle of an AI-assisted task. It answers: \\"Did the GenAI & LLM Documentation process actually work, or did we just generate code fast?\\"","permalink":"/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/05-scenario-scorecards"}],"unlisted":false}}')}}]);