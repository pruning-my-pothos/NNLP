<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-foundations/05-attention-and-transformers/transformer-model-architecture" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Transformer Model Architecture | GenAI &amp; LLM Handbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="genai, llm, documentation, programming, development"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Transformer Model Architecture | GenAI &amp; LLM Handbook"><meta data-rh="true" name="description" content="Dive into the revolutionary Transformer model architecture, which leverages Self-Attention to process sequences in parallel, enabling unprecedented performance and scalability in Natural Language Processing (NLP) and forming the backbone of modern Large Language Models (LLMs)."><meta data-rh="true" property="og:description" content="Dive into the revolutionary Transformer model architecture, which leverages Self-Attention to process sequences in parallel, enabling unprecedented performance and scalability in Natural Language Processing (NLP) and forming the backbone of modern Large Language Models (LLMs)."><link data-rh="true" rel="icon" href="/gen-ai-llm-docs/img/favicon-genai.svg"><link data-rh="true" rel="canonical" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"><link data-rh="true" rel="alternate" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture" hreflang="en"><link data-rh="true" rel="alternate" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Foundations","item":"https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"},{"@type":"ListItem","position":2,"name":"Transformer Model Architecture","item":"https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="GenAI &amp; LLM Handbook" href="/gen-ai-llm-docs/opensearch.xml"><link rel="stylesheet" href="/gen-ai-llm-docs/assets/css/styles.6b77b696.css">
<script src="/gen-ai-llm-docs/assets/js/runtime~main.846e6d39.js" defer="defer"></script>
<script src="/gen-ai-llm-docs/assets/js/main.5795089d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/gen-ai-llm-docs/"><b class="navbar__title text--truncate">GenAI &amp; LLM Handbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/gen-ai-llm-docs/docs/00-handbook-introduction/scope-and-applicability">Start Here</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://www.linkedin.com/in/shailesh-rawat/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Shailesh Rawat · sans_serif_sentiments<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/pruning-my-pothos/gen-ai-llm-docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/gen-ai-llm-docs/docs/00-handbook-introduction/what-is-genai-llm"><span title="Handbook Introduction" class="categoryLinkLabel_W154">Handbook Introduction</span></a><button aria-label="Collapse sidebar category &#x27;Handbook Introduction&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/genai-llm-map"><span title="The GenAI &amp; LLM Documentation Map" class="linkLabel_WmDU">The GenAI &amp; LLM Documentation Map</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/how-to-use-this-repo"><span title="How to Use This Repository" class="linkLabel_WmDU">How to Use This Repository</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/prerequisites-and-entry-criteria"><span title="GenAI &amp; LLM Handbook: Prerequisites and Entry Criteria" class="linkLabel_WmDU">GenAI &amp; LLM Handbook: Prerequisites and Entry Criteria</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/scope-and-applicability"><span title="GenAI &amp; LLM Handbook: Scope and Applicability" class="linkLabel_WmDU">GenAI &amp; LLM Handbook: Scope and Applicability</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/standard-core"><span title="Standard Core (The GenAI &amp; LLM Handbook)" class="linkLabel_WmDU">Standard Core (The GenAI &amp; LLM Handbook)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/style-guide"><span title="Style Guide" class="linkLabel_WmDU">Style Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/what-is-genai-llm"><span title="What is GenAI &amp; LLM?" class="linkLabel_WmDU">What is GenAI &amp; LLM?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/who-this-is-for"><span title="Who This Is For" class="linkLabel_WmDU">Who This Is For</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span title="Foundations" class="categoryLinkLabel_W154">Foundations</span></a><button aria-label="Collapse sidebar category &#x27;Foundations&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span title="Generative AI Basics" class="categoryLinkLabel_W154">Generative AI Basics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/03-nlp-basics/introduction-to-nlp"><span title="NLP Core Concepts" class="categoryLinkLabel_W154">NLP Core Concepts</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/04-sequential-models/introduction-to-sequential-data-and-rnn"><span title="Sequential Models" class="categoryLinkLabel_W154">Sequential Models</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><span title="Attention &amp; Transformers" class="categoryLinkLabel_W154">Attention &amp; Transformers</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><span title="Attention Mechanism" class="linkLabel_WmDU">Attention Mechanism</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"><span title="Transformer Model Architecture" class="linkLabel_WmDU">Transformer Model Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/embeddings-from-language-model"><span title="Embeddings from Language Models" class="linkLabel_WmDU">Embeddings from Language Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/universal-language-model-finetuning-for-text-classification"><span title="Universal Language Model Finetuning (ULMFiT) for Text Classification" class="linkLabel_WmDU">Universal Language Model Finetuning (ULMFiT) for Text Classification</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/generative-pre-training-model-architecture"><span title="Generative Pre-training: Model Architecture" class="linkLabel_WmDU">Generative Pre-training: Model Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece"><span title="Sub-Word Tokenization: BPE, WordPiece, Unigram" class="linkLabel_WmDU">Sub-Word Tokenization: BPE, WordPiece, Unigram</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/bert-model-architecture"><span title="BERT: Model Architecture" class="linkLabel_WmDU">BERT: Model Architecture</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/transformer-architecture"><span title="LLM Deep Dive" class="categoryLinkLabel_W154">LLM Deep Dive</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/gen-ai-llm-docs/docs/01-handbook-core-method/01-overview"><span title="Handbook Method" class="categoryLinkLabel_W154">Handbook Method</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/01-overview"><span title="GenAI Project Lifecycle Overview" class="linkLabel_WmDU">GenAI Project Lifecycle Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/02-ideation-and-use-case"><span title="Ideation and Use Case Definition" class="linkLabel_WmDU">Ideation and Use Case Definition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/03-model-selection"><span title="Model Selection and Tradeoffs" class="linkLabel_WmDU">Model Selection and Tradeoffs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/cost-intuition"><span title="Cost Intuition" class="linkLabel_WmDU">Cost Intuition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/cheat-sheet"><span title="Lifecycle Cheat Sheet" class="linkLabel_WmDU">Lifecycle Cheat Sheet</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/discovery-brief"><span title="Discovery Brief" class="linkLabel_WmDU">Discovery Brief</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/the-genai-llm-loop"><span title="The GenAI &amp; LLM Documentation Loop" class="linkLabel_WmDU">The GenAI &amp; LLM Documentation Loop</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/genai-llm-loop-spec"><span title="GenAI &amp; LLM Documentation Loop: Normative Process Model" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Loop: Normative Process Model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/working-agreements-for-teams"><span title="Working Agreements for Teams" class="linkLabel_WmDU">Working Agreements for Teams</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/intent-spec"><span title="Intent Spec" class="linkLabel_WmDU">Intent Spec</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/constraint-spec"><span title="Constraint Spec" class="linkLabel_WmDU">Constraint Spec</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/delegation-contract"><span title="Delegation Contract" class="linkLabel_WmDU">Delegation Contract</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/generation-requests"><span title="Generation Requests" class="linkLabel_WmDU">Generation Requests</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/review-and-interrogation"><span title="Review and Interrogation" class="linkLabel_WmDU">Review and Interrogation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/acceptance-criteria"><span title="Acceptance Criteria" class="linkLabel_WmDU">Acceptance Criteria</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/iteration-and-release"><span title="Iteration and Release" class="linkLabel_WmDU">Iteration and Release</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/accountability-and-delegation"><span title="GenAI &amp; LLM Documentation Accountability and Delegation Model" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Accountability and Delegation Model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/artifact-contracts"><span title="GenAI &amp; LLM Documentation Artifact Contracts (Normative)" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Artifact Contracts (Normative)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/risks-production-challenges"><span title="Risks &amp; Production Challenges" class="linkLabel_WmDU">Risks &amp; Production Challenges</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/prompt-engineering"><span title="Prompt Engineering" class="linkLabel_WmDU">Prompt Engineering</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/instruction-tuning"><span title="Instruction Tuning" class="linkLabel_WmDU">Instruction Tuning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/fine-tuning"><span title="Fine-tuning" class="linkLabel_WmDU">Fine-tuning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/peft"><span title="Parameter-Efficient Fine-tuning (PEFT)" class="linkLabel_WmDU">Parameter-Efficient Fine-tuning (PEFT)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/05-rag"><span title="Retrieval Augmented Generation (RAG)" class="linkLabel_WmDU">Retrieval Augmented Generation (RAG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/07-tool-use-and-agents"><span title="Tool Use and Agents" class="linkLabel_WmDU">Tool Use and Agents</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/testing-tools"><span title="Testing Tools" class="linkLabel_WmDU">Testing Tools</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/evaluation"><span title="Evaluation" class="linkLabel_WmDU">Evaluation</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/core-skills/00-core-skills-overview"><span title="Core Skills" class="categoryLinkLabel_W154">Core Skills</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/00-eval-overview"><span title="Evaluation Library" class="categoryLinkLabel_W154">Evaluation Library</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/02-execution-patterns/00-pattern-index"><span title="Execution Patterns" class="categoryLinkLabel_W154">Execution Patterns</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/03-professional-scenarios/00-scenarios-index"><span title="Professional Scenarios" class="categoryLinkLabel_W154">Professional Scenarios</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/05-tooling-and-frameworks/00-tooling-index"><span title="Tooling &amp; Frameworks" class="categoryLinkLabel_W154">Tooling &amp; Frameworks</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/04-responsible-ai/01-accountability-and-delegation"><span title="Responsible AI" class="categoryLinkLabel_W154">Responsible AI</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/06-templates/00-templates-index"><span title="Templates" class="categoryLinkLabel_W154">Templates</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/AGENTS"><span title="CLI Agents (General)" class="linkLabel_WmDU">CLI Agents (General)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/CHANGELOG"><span title="Changelog" class="linkLabel_WmDU">Changelog</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/LICENSE"><span title="License" class="linkLabel_WmDU">License</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/gen-ai-llm-docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span>Foundations</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Attention &amp; Transformers</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Transformer Model Architecture</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Transformer Model Architecture</h1></header>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>A Paradigm Shift: From Sequential to Parallel</div><div class="admonitionContent_BuS1"><p>The Transformer is not just an improvement on the RNN; it&#x27;s a complete <strong>paradigm shift</strong>.</p><ul>
<li class=""><strong>RNNs</strong> read a sentence one word at a time, like a person reading through a long, narrow tube. Their understanding of the first word is fuzzy by the time they reach the last.</li>
<li class=""><strong>Transformers</strong> read the entire sentence at once, like a person looking at a whole page. Through <strong>Self-Attention</strong>, every word can directly look at and compare itself to every other word, no matter how far apart they are.</li>
</ul><p>This ability to process in <strong>parallel</strong> rather than <strong>sequentially</strong> is the core breakthrough that made modern LLMs possible.</p></div></div>
<p>The <strong>Transformer</strong> model, introduced in the 2017 paper &quot;Attention Is All You Need&quot; by Vaswani et al., revolutionized sequence modeling and became the dominant architecture in Natural Language Processing (NLP). It completely abandoned Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), relying solely on the <a class="" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism">Attention Mechanism</a> to draw global dependencies between input and output. This breakthrough enabled unprecedented parallelization, leading to faster training times and the ability to handle much longer sequences, paving the way for modern Large Language Models (LLMs).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-innovations-of-the-transformer">Key Innovations of the Transformer<a href="#key-innovations-of-the-transformer" class="hash-link" aria-label="Direct link to Key Innovations of the Transformer" title="Direct link to Key Innovations of the Transformer" translate="no">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>The &quot;Secret Sauce&quot;: Position + Attention</div><div class="admonitionContent_BuS1"><p>The two most important innovations work together to overcome the RNN&#x27;s limitations:</p><ol>
<li class=""><strong>Self-Attention</strong> lets every word look at every other word, so the model understands relationships regardless of distance.</li>
<li class=""><strong>Positional Encodings</strong> give the model a sense of word order.</li>
</ol><p>This combination is the Transformer&#x27;s &quot;secret sauce&quot;: it gets the sequence awareness of an RNN and the parallel processing of a non-sequential model, achieving the best of both worlds.</p></div></div>
<p>The Transformer&#x27;s success stems from several key architectural innovations:</p>
<ol>
<li class=""><strong>Self-Attention (Multi-Head Attention)</strong>: The most crucial component. Instead of processing a sequence word by word, Self-Attention allows the model to weigh the importance of all other words in the input sequence when encoding a single word. Multi-Head Attention applies this process multiple times in parallel, allowing the model to focus on different aspects of relationships.</li>
<li class=""><strong>Positional Encoding</strong>: Since the Transformer does not use recurrence or convolution, it needs a way to account for the order of words in the sequence. Positional encodings are vectors added to the input embeddings that provide information about the absolute or relative position of each token.</li>
<li class=""><strong>Feed-Forward Networks</strong>: Each Encoder and Decoder block contains a simple, position-wise fully connected feed-forward network applied independently to each position.</li>
<li class=""><strong>Layer Normalization and Residual Connections</strong>: Used throughout the network to stabilize training and enable the training of very deep models.</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-transformer-architecture-an-overview">The Transformer Architecture: An Overview<a href="#the-transformer-architecture-an-overview" class="hash-link" aria-label="Direct link to The Transformer Architecture: An Overview" title="Direct link to The Transformer Architecture: An Overview" translate="no">​</a></h2>
<p>The Transformer follows the traditional Encoder-Decoder structure for sequence-to-sequence tasks, but with significant internal differences.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="visual-suggestion-high-level-transformer-architecture">Visual Suggestion: High-Level Transformer Architecture<a href="#visual-suggestion-high-level-transformer-architecture" class="hash-link" aria-label="Direct link to Visual Suggestion: High-Level Transformer Architecture" title="Direct link to Visual Suggestion: High-Level Transformer Architecture" translate="no">​</a></h3>
<!-- -->
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-the-encoder-stack">1. The Encoder Stack<a href="#1-the-encoder-stack" class="hash-link" aria-label="Direct link to 1. The Encoder Stack" title="Direct link to 1. The Encoder Stack" translate="no">​</a></h3>
<p>The encoder is responsible for mapping an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $(z_1, ..., z_n)$.</p>
<ul>
<li class="">It consists of a stack of $N$ identical layers (e.g., $N=6$).</li>
<li class="">Each layer has two sub-layers:<!-- -->
<ol>
<li class=""><strong>Multi-Head Self-Attention Mechanism</strong>: Computes attention over the input sequence itself.</li>
<li class=""><strong>Position-wise Feed-Forward Network</strong>: A simple fully connected network.</li>
</ol>
</li>
<li class="">Each sub-layer is followed by a <strong>Residual Connection</strong> and <strong>Layer Normalization</strong>.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-the-decoder-stack">2. The Decoder Stack<a href="#2-the-decoder-stack" class="hash-link" aria-label="Direct link to 2. The Decoder Stack" title="Direct link to 2. The Decoder Stack" translate="no">​</a></h3>
<p>The decoder is responsible for taking the encoder&#x27;s output and the previously generated symbols to generate the output sequence $(y_1, ..., y_m)$.</p>
<ul>
<li class="">It also consists of a stack of $N$ identical layers.</li>
<li class="">Each layer has three sub-layers:<!-- -->
<ol>
<li class=""><strong>Masked Multi-Head Self-Attention</strong>: Similar to the encoder&#x27;s self-attention, but it&#x27;s &quot;masked&quot; to prevent attending to future positions (to ensure prediction depends only on known outputs).</li>
<li class=""><strong>Multi-Head Attention over Encoder Outputs</strong>: This layer performs attention over the output of the <em>encoder stack</em>, allowing the decoder to focus on relevant parts of the input sequence.</li>
<li class=""><strong>Position-wise Feed-Forward Network</strong>: Same as in the encoder.</li>
</ol>
</li>
<li class="">Again, each sub-layer is followed by a <strong>Residual Connection</strong> and <strong>Layer Normalization</strong>.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="deeper-dive-multi-head-self-attention">Deeper Dive: Multi-Head Self-Attention<a href="#deeper-dive-multi-head-self-attention" class="hash-link" aria-label="Direct link to Deeper Dive: Multi-Head Self-Attention" title="Direct link to Deeper Dive: Multi-Head Self-Attention" translate="no">​</a></h2>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Analogy: A Committee of Experts</div><div class="admonitionContent_BuS1"><p><strong>Self-Attention</strong> is like one person reading a sentence to understand its internal references.</p><p><strong>Multi-Head Attention</strong> is like a committee of experts reading the sentence simultaneously. Each &quot;head&quot; is an expert in a different type of relationship:</p><ul>
<li class=""><strong>Head 1 (The Linguist):</strong> Looks at grammatical structure.</li>
<li class=""><strong>Head 2 (The Historian):</strong> Looks for connections to known facts.</li>
<li class=""><strong>Head 3 (The Storyteller):</strong> Looks for narrative flow.</li>
<li class="">...and so on.</li>
</ul><p>By combining the perspectives of all these experts, the model gets a much richer and more nuanced understanding of the text.</p></div></div>
<p>Self-Attention computes a weighted sum of all values in the input sequence, where the weight assigned to each value is determined by a &quot;query&quot; and a &quot;key&quot; vector derived from each word.</p>
<p>The core idea: For each word, calculate three vectors:</p>
<ul>
<li class=""><strong>Query (Q)</strong>: Represents what information this word is looking for.</li>
<li class=""><strong>Key (K)</strong>: Represents what information this word offers.</li>
<li class=""><strong>Value (V)</strong>: The actual information content of this word.</li>
</ul>
<p>The attention score is typically calculated as <code>softmax((Q * K^T) / sqrt(d_k)) * V</code>.</p>
<p><strong>Multi-Head Attention</strong> extends this by performing the attention function <code>h</code> times in parallel with different learned linear projections to Query, Key, and Value. The outputs from these <code>h</code> attention heads are then concatenated and linearly transformed into the final output. This allows the model to learn different types of relationships (e.g., syntactic, semantic) simultaneously.</p>
<p><strong>Actionable Insight</strong>: The ability of Self-Attention to capture global dependencies in a single step (unlike RNNs that process sequentially) is what makes Transformers so powerful for long-range context.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="positional-encoding-explained">Positional Encoding Explained<a href="#positional-encoding-explained" class="hash-link" aria-label="Direct link to Positional Encoding Explained" title="Direct link to Positional Encoding Explained" translate="no">​</a></h2>
<p>Without recurrence or convolutions, a Transformer would treat all words in a sequence as an unordered set (like a Bag-of-Words model). Positional encodings provide the necessary sequential information.</p>
<ul>
<li class=""><strong>How it Works</strong>: These are sine and cosine functions of different frequencies. They are added directly to the input embeddings (after converting words to vectors).</li>
<li class=""><strong>Why it works</strong>: This allows the model to infer the relative positions of words and integrate this information into the attention mechanism.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="relevance-to-generative-ai-and-llms">Relevance to Generative AI and LLMs<a href="#relevance-to-generative-ai-and-llms" class="hash-link" aria-label="Direct link to Relevance to Generative AI and LLMs" title="Direct link to Relevance to Generative AI and LLMs" translate="no">​</a></h2>
<p>The Transformer architecture is the <strong>foundational technology</strong> for almost all modern Large Language Models (LLMs). Its parallel processing capabilities, combined with the power of self-attention to capture complex, long-range dependencies, enabled models to scale to unprecedented sizes and training data volumes.</p>
<ul>
<li class="">
<p><strong>BERT (Encoder-only)</strong>: Models like BERT use only the Transformer&#x27;s encoder stack for tasks like text classification, question answering, and named entity recognition.</p>
</li>
<li class="">
<p><strong>GPT (Decoder-only)</strong>: Generative models like OpenAI&#x27;s GPT series primarily use a modified version of the Transformer&#x27;s decoder stack (without the encoder-attention layer), focusing on autoregressive generation (predicting the next token based on previous ones).</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Why &quot;Decoder-Only&quot;?</div><div class="admonitionContent_BuS1"><p>This is a common point of confusion. A model like GPT is called &quot;decoder-only&quot; because its primary job is to <strong>generate text</strong> by predicting the next word. It doesn&#x27;t need to translate from a <em>different</em> source language.</p><ul>
<li class="">It still uses the core components of a Transformer decoder: <strong>Masked Self-Attention</strong> to look at the previous words it has already written.</li>
<li class="">It lacks the second <strong>Multi-Head Attention</strong> layer that would have looked at an encoder&#x27;s output, because there <em>is</em> no encoder.
Its only &quot;source&quot; is the text it has generated so far.</li>
</ul></div></div>
</li>
</ul>
<p>The Transformer&#x27;s ability to &quot;see&quot; the entire sequence at once (or at least, to quickly compute relationships across it) is what gives LLMs their remarkable contextual understanding and generative fluency.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">​</a></h2>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="now-that-you-understand-the-architecture-well-explore-how-these-models-create-rich-representations-of-language-in-embeddings-from-language-model">Now that you understand the architecture, we&#x27;ll explore how these models create rich representations of language in <a class="" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/embeddings-from-language-model">Embeddings from Language Model</a>.<a href="#now-that-you-understand-the-architecture-well-explore-how-these-models-create-rich-representations-of-language-in-embeddings-from-language-model" class="hash-link" aria-label="Direct link to now-that-you-understand-the-architecture-well-explore-how-these-models-create-rich-representations-of-language-in-embeddings-from-language-model" title="Direct link to now-that-you-understand-the-architecture-well-explore-how-these-models-create-rich-representations-of-language-in-embeddings-from-language-model" translate="no">​</a></h2></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Attention Mechanism</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/embeddings-from-language-model"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Embeddings from Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#key-innovations-of-the-transformer" class="table-of-contents__link toc-highlight">Key Innovations of the Transformer</a></li><li><a href="#the-transformer-architecture-an-overview" class="table-of-contents__link toc-highlight">The Transformer Architecture: An Overview</a><ul><li><a href="#visual-suggestion-high-level-transformer-architecture" class="table-of-contents__link toc-highlight">Visual Suggestion: High-Level Transformer Architecture</a></li><li><a href="#1-the-encoder-stack" class="table-of-contents__link toc-highlight">1. The Encoder Stack</a></li><li><a href="#2-the-decoder-stack" class="table-of-contents__link toc-highlight">2. The Decoder Stack</a></li></ul></li><li><a href="#deeper-dive-multi-head-self-attention" class="table-of-contents__link toc-highlight">Deeper Dive: Multi-Head Self-Attention</a></li><li><a href="#positional-encoding-explained" class="table-of-contents__link toc-highlight">Positional Encoding Explained</a></li><li><a href="#relevance-to-generative-ai-and-llms" class="table-of-contents__link toc-highlight">Relevance to Generative AI and LLMs</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li><li><a href="#now-that-you-understand-the-architecture-well-explore-how-these-models-create-rich-representations-of-language-in-embeddings-from-language-model" class="table-of-contents__link toc-highlight">Now that you understand the architecture, we&#39;ll explore how these models create rich representations of language in Embeddings from Language Model.</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">GenAI & LLM Handbook · 2025</div></div></div></footer></div>
</body>
</html>