<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Sub-Word Tokenization: BPE, WordPiece, Unigram | GenAI &amp; LLM Handbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="genai, llm, documentation, programming, development"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Sub-Word Tokenization: BPE, WordPiece, Unigram | GenAI &amp; LLM Handbook"><meta data-rh="true" name="description" content="Understand the importance of sub-word tokenization strategies like Byte Pair Encoding (BPE), WordPiece, and Unigram Language Model (ULM) in handling large vocabularies, rare words, and out-of-vocabulary (OOV) terms, crucial for modern Large Language Models (LLMs)."><meta data-rh="true" property="og:description" content="Understand the importance of sub-word tokenization strategies like Byte Pair Encoding (BPE), WordPiece, and Unigram Language Model (ULM) in handling large vocabularies, rare words, and out-of-vocabulary (OOV) terms, crucial for modern Large Language Models (LLMs)."><link data-rh="true" rel="icon" href="/gen-ai-llm-docs/img/favicon-genai.svg"><link data-rh="true" rel="canonical" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece"><link data-rh="true" rel="alternate" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece" hreflang="en"><link data-rh="true" rel="alternate" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Foundations","item":"https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"},{"@type":"ListItem","position":2,"name":"Sub-Word Tokenization: BPE, WordPiece, Unigram","item":"https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="GenAI &amp; LLM Handbook" href="/gen-ai-llm-docs/opensearch.xml"><link rel="stylesheet" href="/gen-ai-llm-docs/assets/css/styles.6b77b696.css">
<script src="/gen-ai-llm-docs/assets/js/runtime~main.846e6d39.js" defer="defer"></script>
<script src="/gen-ai-llm-docs/assets/js/main.5795089d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/gen-ai-llm-docs/"><b class="navbar__title text--truncate">GenAI &amp; LLM Handbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/gen-ai-llm-docs/docs/00-handbook-introduction/scope-and-applicability">Start Here</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://www.linkedin.com/in/shailesh-rawat/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Shailesh Rawat · sans_serif_sentiments<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/pruning-my-pothos/gen-ai-llm-docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/gen-ai-llm-docs/docs/00-handbook-introduction/what-is-genai-llm"><span title="Handbook Introduction" class="categoryLinkLabel_W154">Handbook Introduction</span></a><button aria-label="Collapse sidebar category &#x27;Handbook Introduction&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/genai-llm-map"><span title="The GenAI &amp; LLM Documentation Map" class="linkLabel_WmDU">The GenAI &amp; LLM Documentation Map</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/how-to-use-this-repo"><span title="How to Use This Repository" class="linkLabel_WmDU">How to Use This Repository</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/prerequisites-and-entry-criteria"><span title="GenAI &amp; LLM Handbook: Prerequisites and Entry Criteria" class="linkLabel_WmDU">GenAI &amp; LLM Handbook: Prerequisites and Entry Criteria</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/scope-and-applicability"><span title="GenAI &amp; LLM Handbook: Scope and Applicability" class="linkLabel_WmDU">GenAI &amp; LLM Handbook: Scope and Applicability</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/standard-core"><span title="Standard Core (The GenAI &amp; LLM Handbook)" class="linkLabel_WmDU">Standard Core (The GenAI &amp; LLM Handbook)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/style-guide"><span title="Style Guide" class="linkLabel_WmDU">Style Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/what-is-genai-llm"><span title="What is GenAI &amp; LLM?" class="linkLabel_WmDU">What is GenAI &amp; LLM?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/who-this-is-for"><span title="Who This Is For" class="linkLabel_WmDU">Who This Is For</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span title="Foundations" class="categoryLinkLabel_W154">Foundations</span></a><button aria-label="Collapse sidebar category &#x27;Foundations&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span title="Generative AI Basics" class="categoryLinkLabel_W154">Generative AI Basics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/03-nlp-basics/introduction-to-nlp"><span title="NLP Core Concepts" class="categoryLinkLabel_W154">NLP Core Concepts</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/04-sequential-models/introduction-to-sequential-data-and-rnn"><span title="Sequential Models" class="categoryLinkLabel_W154">Sequential Models</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><span title="Attention &amp; Transformers" class="categoryLinkLabel_W154">Attention &amp; Transformers</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><span title="Attention Mechanism" class="linkLabel_WmDU">Attention Mechanism</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"><span title="Transformer Model Architecture" class="linkLabel_WmDU">Transformer Model Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/embeddings-from-language-model"><span title="Embeddings from Language Models" class="linkLabel_WmDU">Embeddings from Language Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/universal-language-model-finetuning-for-text-classification"><span title="Universal Language Model Finetuning (ULMFiT) for Text Classification" class="linkLabel_WmDU">Universal Language Model Finetuning (ULMFiT) for Text Classification</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/generative-pre-training-model-architecture"><span title="Generative Pre-training: Model Architecture" class="linkLabel_WmDU">Generative Pre-training: Model Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece"><span title="Sub-Word Tokenization: BPE, WordPiece, Unigram" class="linkLabel_WmDU">Sub-Word Tokenization: BPE, WordPiece, Unigram</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/bert-model-architecture"><span title="BERT: Model Architecture" class="linkLabel_WmDU">BERT: Model Architecture</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/transformer-architecture"><span title="LLM Deep Dive" class="categoryLinkLabel_W154">LLM Deep Dive</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/gen-ai-llm-docs/docs/01-handbook-core-method/01-overview"><span title="Handbook Method" class="categoryLinkLabel_W154">Handbook Method</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/01-overview"><span title="GenAI Project Lifecycle Overview" class="linkLabel_WmDU">GenAI Project Lifecycle Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/02-ideation-and-use-case"><span title="Ideation and Use Case Definition" class="linkLabel_WmDU">Ideation and Use Case Definition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/03-model-selection"><span title="Model Selection and Tradeoffs" class="linkLabel_WmDU">Model Selection and Tradeoffs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/cost-intuition"><span title="Cost Intuition" class="linkLabel_WmDU">Cost Intuition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/cheat-sheet"><span title="Lifecycle Cheat Sheet" class="linkLabel_WmDU">Lifecycle Cheat Sheet</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/discovery-brief"><span title="Discovery Brief" class="linkLabel_WmDU">Discovery Brief</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/the-genai-llm-loop"><span title="The GenAI &amp; LLM Documentation Loop" class="linkLabel_WmDU">The GenAI &amp; LLM Documentation Loop</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/genai-llm-loop-spec"><span title="GenAI &amp; LLM Documentation Loop: Normative Process Model" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Loop: Normative Process Model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/working-agreements-for-teams"><span title="Working Agreements for Teams" class="linkLabel_WmDU">Working Agreements for Teams</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/intent-spec"><span title="Intent Spec" class="linkLabel_WmDU">Intent Spec</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/constraint-spec"><span title="Constraint Spec" class="linkLabel_WmDU">Constraint Spec</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/delegation-contract"><span title="Delegation Contract" class="linkLabel_WmDU">Delegation Contract</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/generation-requests"><span title="Generation Requests" class="linkLabel_WmDU">Generation Requests</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/review-and-interrogation"><span title="Review and Interrogation" class="linkLabel_WmDU">Review and Interrogation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/acceptance-criteria"><span title="Acceptance Criteria" class="linkLabel_WmDU">Acceptance Criteria</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/iteration-and-release"><span title="Iteration and Release" class="linkLabel_WmDU">Iteration and Release</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/accountability-and-delegation"><span title="GenAI &amp; LLM Documentation Accountability and Delegation Model" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Accountability and Delegation Model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/artifact-contracts"><span title="GenAI &amp; LLM Documentation Artifact Contracts (Normative)" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Artifact Contracts (Normative)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/risks-production-challenges"><span title="Risks &amp; Production Challenges" class="linkLabel_WmDU">Risks &amp; Production Challenges</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/prompt-engineering"><span title="Prompt Engineering" class="linkLabel_WmDU">Prompt Engineering</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/instruction-tuning"><span title="Instruction Tuning" class="linkLabel_WmDU">Instruction Tuning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/fine-tuning"><span title="Fine-tuning" class="linkLabel_WmDU">Fine-tuning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/peft"><span title="Parameter-Efficient Fine-tuning (PEFT)" class="linkLabel_WmDU">Parameter-Efficient Fine-tuning (PEFT)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/05-rag"><span title="Retrieval Augmented Generation (RAG)" class="linkLabel_WmDU">Retrieval Augmented Generation (RAG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/07-tool-use-and-agents"><span title="Tool Use and Agents" class="linkLabel_WmDU">Tool Use and Agents</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/testing-tools"><span title="Testing Tools" class="linkLabel_WmDU">Testing Tools</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/evaluation"><span title="Evaluation" class="linkLabel_WmDU">Evaluation</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/core-skills/00-core-skills-overview"><span title="Core Skills" class="categoryLinkLabel_W154">Core Skills</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/00-eval-overview"><span title="Evaluation Library" class="categoryLinkLabel_W154">Evaluation Library</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/02-execution-patterns/00-pattern-index"><span title="Execution Patterns" class="categoryLinkLabel_W154">Execution Patterns</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/03-professional-scenarios/00-scenarios-index"><span title="Professional Scenarios" class="categoryLinkLabel_W154">Professional Scenarios</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/05-tooling-and-frameworks/00-tooling-index"><span title="Tooling &amp; Frameworks" class="categoryLinkLabel_W154">Tooling &amp; Frameworks</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/04-responsible-ai/01-accountability-and-delegation"><span title="Responsible AI" class="categoryLinkLabel_W154">Responsible AI</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/06-templates/00-templates-index"><span title="Templates" class="categoryLinkLabel_W154">Templates</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/AGENTS"><span title="CLI Agents (General)" class="linkLabel_WmDU">CLI Agents (General)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/CHANGELOG"><span title="Changelog" class="linkLabel_WmDU">Changelog</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/LICENSE"><span title="License" class="linkLabel_WmDU">License</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/gen-ai-llm-docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span>Foundations</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Attention &amp; Transformers</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Sub-Word Tokenization: BPE, WordPiece, Unigram</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Sub-Word Tokenization: BPE, WordPiece, Unigram</h1></header>
<p>In Natural Language Processing (NLP), <strong>tokenization</strong> is the process of breaking down raw text into smaller units called tokens. While basic tokenization often splits text by words, this approach faces significant challenges when dealing with:</p>
<ul>
<li class=""><strong>Large Vocabularies</strong>: Many unique words require a huge vocabulary, increasing model size and complexity.</li>
<li class=""><strong>Rare Words</strong>: Words that appear infrequently might not have enough examples for their embeddings to be learned effectively.</li>
<li class=""><strong>Out-of-Vocabulary (OOV) Words</strong>: Words not seen during training cannot be represented by the model.</li>
</ul>
<p><strong>Sub-word tokenization</strong> strategies address these issues by breaking down words into meaningful sub-word units (e.g., prefixes, suffixes, root forms). This allows models to represent rare and OOV words by composing them from known sub-word units, and to manage vocabulary size more efficiently. These techniques are fundamental for the <a class="" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture">Transformer Architecture</a> and modern Large Language Models (LLMs).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-need-for-sub-word-tokenization">The Need for Sub-Word Tokenization<a href="#the-need-for-sub-word-tokenization" class="hash-link" aria-label="Direct link to The Need for Sub-Word Tokenization" title="Direct link to The Need for Sub-Word Tokenization" translate="no">​</a></h2>
<p>Consider the word &quot;unbelievable.&quot; A word-level tokenizer would treat this as one token. If the model hasn&#x27;t seen it before, it&#x27;s an OOV word. A sub-word tokenizer might break it into &quot;un&quot;, &quot;believe&quot;, &quot;able&quot; – all of which are common sub-word units the model likely knows.</p>
<p><strong>Actionable Insight</strong>: Sub-word tokenization strikes a balance between character-level and word-level tokenization. It keeps the vocabulary size manageable while allowing the model to generalize to unseen words.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="popular-sub-word-tokenization-algorithms">Popular Sub-Word Tokenization Algorithms<a href="#popular-sub-word-tokenization-algorithms" class="hash-link" aria-label="Direct link to Popular Sub-Word Tokenization Algorithms" title="Direct link to Popular Sub-Word Tokenization Algorithms" translate="no">​</a></h2>
<p>The most widely used sub-word tokenization algorithms are:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-byte-pair-encoding-bpe">1. Byte Pair Encoding (BPE)<a href="#1-byte-pair-encoding-bpe" class="hash-link" aria-label="Direct link to 1. Byte Pair Encoding (BPE)" title="Direct link to 1. Byte Pair Encoding (BPE)" translate="no">​</a></h3>
<ul>
<li class=""><strong>Origin</strong>: Originally a data compression algorithm.</li>
<li class=""><strong>How it Works</strong>:<!-- -->
<ol>
<li class="">Start with a vocabulary of individual characters present in the training data.</li>
<li class="">Iteratively count the frequency of all adjacent character pairs and merge the most frequent pair into a new sub-word unit.</li>
<li class="">Repeat until a predefined vocabulary size is reached or no more merges can be made.</li>
</ol>
</li>
<li class=""><strong>Example</strong>:<!-- -->
<ul>
<li class="">Initial: <code>{&quot;t&quot;, &quot;h&quot;, &quot;e&quot;, &quot;q&quot;, &quot;u&quot;, &quot;i&quot;, &quot;c&quot;, &quot;k&quot;, &quot;b&quot;, &quot;r&quot;, &quot;o&quot;, &quot;w&quot;, &quot;n&quot;, &quot;f&quot;, &quot;x&quot;}</code></li>
<li class="">Merge &quot;th&quot;: <code>{&quot;th&quot;, &quot;e&quot;, ...}</code></li>
<li class="">Merge &quot;qu&quot;: <code>{&quot;th&quot;, &quot;e&quot;, &quot;qu&quot;, ...}</code></li>
<li class="">... Eventually forms &quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;</li>
<li class="">For an OOV word like &quot;lowest&quot;: It might break into &quot;low&quot;, &quot;est&quot;.</li>
</ul>
</li>
<li class=""><strong>Models Using BPE</strong>: GPT-2, GPT-3, RoBERTa.</li>
<li class=""><strong>Actionable Insight</strong>: BPE is effective and simple, allowing open-vocabulary models without an infinitely large vocabulary.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-wordpiece">2. WordPiece<a href="#2-wordpiece" class="hash-link" aria-label="Direct link to 2. WordPiece" title="Direct link to 2. WordPiece" translate="no">​</a></h3>
<ul>
<li class=""><strong>Origin</strong>: Used in Google&#x27;s neural machine translation systems and later by BERT.</li>
<li class=""><strong>How it Works</strong>: Similar to BPE but with a key difference in the merge criterion. Instead of merging the most frequent pair, WordPiece merges the pair that, when combined, maximizes the likelihood of the training data. It considers the probability of a word forming from its sub-words.<!-- -->
<ol>
<li class="">Start with a vocabulary of individual characters and all words in the training corpus.</li>
<li class="">Iteratively evaluate all possible merges. Select the merge that results in the largest increase in the log-likelihood of the training data when added to the vocabulary.</li>
<li class="">Repeat until the desired vocabulary size is reached.</li>
</ol>
</li>
<li class=""><strong>Example</strong>: &quot;unwanted&quot; -&gt; &quot;un&quot;, &quot;##want&quot;, &quot;##ed&quot;. The <code>##</code> prefix indicates a sub-word unit that is not the beginning of a word.</li>
<li class=""><strong>Models Using WordPiece</strong>: BERT, DistilBERT.</li>
<li class=""><strong>Actionable Insight</strong>: WordPiece tends to create tokens that are more linguistically meaningful than basic BPE, often resulting in better performance for models like BERT.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-unigram-language-model-ulm-tokenization">3. Unigram Language Model (ULM) Tokenization<a href="#3-unigram-language-model-ulm-tokenization" class="hash-link" aria-label="Direct link to 3. Unigram Language Model (ULM) Tokenization" title="Direct link to 3. Unigram Language Model (ULM) Tokenization" translate="no">​</a></h3>
<ul>
<li class=""><strong>Origin</strong>: Used by SentencePiece, common in Transformer models like XLNet and T5.</li>
<li class=""><strong>How it Works</strong>: Unlike BPE and WordPiece (which are greedy bottom-up approaches), Unigram LM tokenization is a top-down probabilistic approach.<!-- -->
<ol>
<li class="">Starts with a large vocabulary of possible sub-word units (initially all characters and words).</li>
<li class="">Trains a unigram language model on this vocabulary.</li>
<li class="">Iteratively prunes the vocabulary by removing sub-words that have a low impact on the overall likelihood of the training data, until a desired vocabulary size is met. It can generate multiple segmentations for a sentence, and then chooses the most probable one.</li>
</ol>
</li>
<li class=""><strong>Example</strong>: &quot;beautiful&quot; might be tokenized as &quot;beau&quot;, &quot;tiful&quot; or &quot;be&quot;, &quot;aut&quot;, &quot;iful&quot;.</li>
<li class=""><strong>Models Using ULM</strong>: ALBERT, T5, XLNet.</li>
<li class=""><strong>Actionable Insight</strong>: Unigram LM models allow for multiple valid tokenizations of a word, which can be useful for ambiguous cases, and are often more robust across different languages.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="actionable-insight-tokenization-choice-matters">Actionable Insight: Tokenization Choice Matters<a href="#actionable-insight-tokenization-choice-matters" class="hash-link" aria-label="Direct link to Actionable Insight: Tokenization Choice Matters" title="Direct link to Actionable Insight: Tokenization Choice Matters" translate="no">​</a></h2>
<p>The choice of tokenizer significantly impacts model performance, vocabulary size, and how out-of-vocabulary words are handled. When working with LLMs:</p>
<ul>
<li class=""><strong>Understand the Tokenizer</strong>: Always know which tokenizer was used to train your LLM. Using a different tokenizer during inference will lead to poor results.</li>
<li class=""><strong>Context Window</strong>: Tokenizers directly affect the number of tokens a given sequence consumes, which directly impacts the effective context window and cost of LLM inference.</li>
<li class=""><strong>Special Tokens</strong>: Be aware of special tokens (e.g., <code>[CLS]</code>, <code>[SEP]</code>, <code>&lt;pad&gt;</code>, <code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code>) that tokenizers add for model-specific processing.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="visual-suggestion-tokenization-comparison">Visual Suggestion: Tokenization Comparison<a href="#visual-suggestion-tokenization-comparison" class="hash-link" aria-label="Direct link to Visual Suggestion: Tokenization Comparison" title="Direct link to Visual Suggestion: Tokenization Comparison" translate="no">​</a></h2>
<!-- -->
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="relevance-to-generative-ai-and-llms">Relevance to Generative AI and LLMs<a href="#relevance-to-generative-ai-and-llms" class="hash-link" aria-label="Direct link to Relevance to Generative AI and LLMs" title="Direct link to Relevance to Generative AI and LLMs" translate="no">​</a></h2>
<p>Sub-word tokenization is indispensable for modern LLMs. It&#x27;s how these models efficiently handle the vast complexity of human language, balancing the need for a rich vocabulary with computational feasibility. By breaking words into smaller, reusable units, LLMs can:</p>
<ul>
<li class=""><strong>Handle OOV words</strong>: Generate representations for words they&#x27;ve never seen before.</li>
<li class=""><strong>Manage vocabulary size</strong>: Keep the total number of unique tokens manageable.</li>
<li class=""><strong>Learn morphology</strong>: Implicitly learn about word structure (prefixes, suffixes, roots).</li>
<li class=""><strong>Improve cross-lingual transfer</strong>: Sub-word units are often shared across related languages.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">​</a></h2>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="with-a-solid-understanding-of-tokenization-we-can-now-look-at-how-these-representations-are-used-in-the-context-of-specific-landmark-models-starting-with-bert-model-architecture">With a solid understanding of tokenization, we can now look at how these representations are used in the context of specific landmark models, starting with <strong><a class="" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/bert-model-architecture">BERT: Model Architecture</a></strong>.<a href="#with-a-solid-understanding-of-tokenization-we-can-now-look-at-how-these-representations-are-used-in-the-context-of-specific-landmark-models-starting-with-bert-model-architecture" class="hash-link" aria-label="Direct link to with-a-solid-understanding-of-tokenization-we-can-now-look-at-how-these-representations-are-used-in-the-context-of-specific-landmark-models-starting-with-bert-model-architecture" title="Direct link to with-a-solid-understanding-of-tokenization-we-can-now-look-at-how-these-representations-are-used-in-the-context-of-specific-landmark-models-starting-with-bert-model-architecture" translate="no">​</a></h2></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/generative-pre-training-model-architecture"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Generative Pre-training: Model Architecture</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/bert-model-architecture"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">BERT: Model Architecture</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-need-for-sub-word-tokenization" class="table-of-contents__link toc-highlight">The Need for Sub-Word Tokenization</a></li><li><a href="#popular-sub-word-tokenization-algorithms" class="table-of-contents__link toc-highlight">Popular Sub-Word Tokenization Algorithms</a><ul><li><a href="#1-byte-pair-encoding-bpe" class="table-of-contents__link toc-highlight">1. Byte Pair Encoding (BPE)</a></li><li><a href="#2-wordpiece" class="table-of-contents__link toc-highlight">2. WordPiece</a></li><li><a href="#3-unigram-language-model-ulm-tokenization" class="table-of-contents__link toc-highlight">3. Unigram Language Model (ULM) Tokenization</a></li></ul></li><li><a href="#actionable-insight-tokenization-choice-matters" class="table-of-contents__link toc-highlight">Actionable Insight: Tokenization Choice Matters</a></li><li><a href="#visual-suggestion-tokenization-comparison" class="table-of-contents__link toc-highlight">Visual Suggestion: Tokenization Comparison</a></li><li><a href="#relevance-to-generative-ai-and-llms" class="table-of-contents__link toc-highlight">Relevance to Generative AI and LLMs</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li><li><a href="#with-a-solid-understanding-of-tokenization-we-can-now-look-at-how-these-representations-are-used-in-the-context-of-specific-landmark-models-starting-with-bert-model-architecture" class="table-of-contents__link toc-highlight">With a solid understanding of tokenization, we can now look at how these representations are used in the context of specific landmark models, starting with <strong>BERT: Model Architecture</strong>.</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">GenAI & LLM Handbook · 2025</div></div></div></footer></div>
</body>
</html>