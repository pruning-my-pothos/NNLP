<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-foundations/05-attention-and-transformers/attention-mechanism" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Attention Mechanism | GenAI &amp; LLM Handbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="genai, llm, documentation, programming, development"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Attention Mechanism | GenAI &amp; LLM Handbook"><meta data-rh="true" name="description" content="Understand the groundbreaking Attention Mechanism, a key innovation that revolutionized sequence modeling by allowing models to dynamically focus on relevant parts of an input sequence, overcoming the limitations of fixed-size context vectors in RNNs."><meta data-rh="true" property="og:description" content="Understand the groundbreaking Attention Mechanism, a key innovation that revolutionized sequence modeling by allowing models to dynamically focus on relevant parts of an input sequence, overcoming the limitations of fixed-size context vectors in RNNs."><link data-rh="true" rel="icon" href="/gen-ai-llm-docs/img/favicon-genai.svg"><link data-rh="true" rel="canonical" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><link data-rh="true" rel="alternate" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism" hreflang="en"><link data-rh="true" rel="alternate" href="https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Foundations","item":"https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"},{"@type":"ListItem","position":2,"name":"Attention Mechanism","item":"https://pruning-my-pothos.github.io/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="GenAI &amp; LLM Handbook" href="/gen-ai-llm-docs/opensearch.xml"><link rel="stylesheet" href="/gen-ai-llm-docs/assets/css/styles.6b77b696.css">
<script src="/gen-ai-llm-docs/assets/js/runtime~main.846e6d39.js" defer="defer"></script>
<script src="/gen-ai-llm-docs/assets/js/main.5795089d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/gen-ai-llm-docs/"><b class="navbar__title text--truncate">GenAI &amp; LLM Handbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/gen-ai-llm-docs/docs/00-handbook-introduction/scope-and-applicability">Start Here</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://www.linkedin.com/in/shailesh-rawat/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Shailesh Rawat · sans_serif_sentiments<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/pruning-my-pothos/gen-ai-llm-docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/gen-ai-llm-docs/docs/00-handbook-introduction/what-is-genai-llm"><span title="Handbook Introduction" class="categoryLinkLabel_W154">Handbook Introduction</span></a><button aria-label="Collapse sidebar category &#x27;Handbook Introduction&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/genai-llm-map"><span title="The GenAI &amp; LLM Documentation Map" class="linkLabel_WmDU">The GenAI &amp; LLM Documentation Map</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/glossary"><span title="Glossary" class="linkLabel_WmDU">Glossary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/how-to-use-this-repo"><span title="How to Use This Repository" class="linkLabel_WmDU">How to Use This Repository</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/prerequisites-and-entry-criteria"><span title="GenAI &amp; LLM Handbook: Prerequisites and Entry Criteria" class="linkLabel_WmDU">GenAI &amp; LLM Handbook: Prerequisites and Entry Criteria</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/scope-and-applicability"><span title="GenAI &amp; LLM Handbook: Scope and Applicability" class="linkLabel_WmDU">GenAI &amp; LLM Handbook: Scope and Applicability</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/standard-core"><span title="Standard Core (The GenAI &amp; LLM Handbook)" class="linkLabel_WmDU">Standard Core (The GenAI &amp; LLM Handbook)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/style-guide"><span title="Style Guide" class="linkLabel_WmDU">Style Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/what-is-genai-llm"><span title="What is GenAI &amp; LLM?" class="linkLabel_WmDU">What is GenAI &amp; LLM?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/00-handbook-introduction/who-this-is-for"><span title="Who This Is For" class="linkLabel_WmDU">Who This Is For</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span title="Foundations" class="categoryLinkLabel_W154">Foundations</span></a><button aria-label="Collapse sidebar category &#x27;Foundations&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span title="Generative AI Basics" class="categoryLinkLabel_W154">Generative AI Basics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/03-nlp-basics/introduction-to-nlp"><span title="NLP Core Concepts" class="categoryLinkLabel_W154">NLP Core Concepts</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/04-sequential-models/introduction-to-sequential-data-and-rnn"><span title="Sequential Models" class="categoryLinkLabel_W154">Sequential Models</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><span title="Attention &amp; Transformers" class="categoryLinkLabel_W154">Attention &amp; Transformers</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/attention-mechanism"><span title="Attention Mechanism" class="linkLabel_WmDU">Attention Mechanism</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"><span title="Transformer Model Architecture" class="linkLabel_WmDU">Transformer Model Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/embeddings-from-language-model"><span title="Embeddings from Language Models" class="linkLabel_WmDU">Embeddings from Language Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/universal-language-model-finetuning-for-text-classification"><span title="Universal Language Model Finetuning (ULMFiT) for Text Classification" class="linkLabel_WmDU">Universal Language Model Finetuning (ULMFiT) for Text Classification</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/generative-pre-training-model-architecture"><span title="Generative Pre-training: Model Architecture" class="linkLabel_WmDU">Generative Pre-training: Model Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/sub-word-tokenization-bpe-wordpiece"><span title="Sub-Word Tokenization: BPE, WordPiece, Unigram" class="linkLabel_WmDU">Sub-Word Tokenization: BPE, WordPiece, Unigram</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/bert-model-architecture"><span title="BERT: Model Architecture" class="linkLabel_WmDU">BERT: Model Architecture</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/foundations/02-llm-deep-dive/transformer-architecture"><span title="LLM Deep Dive" class="categoryLinkLabel_W154">LLM Deep Dive</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/gen-ai-llm-docs/docs/01-handbook-core-method/01-overview"><span title="Handbook Method" class="categoryLinkLabel_W154">Handbook Method</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/01-overview"><span title="GenAI Project Lifecycle Overview" class="linkLabel_WmDU">GenAI Project Lifecycle Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/02-ideation-and-use-case"><span title="Ideation and Use Case Definition" class="linkLabel_WmDU">Ideation and Use Case Definition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/03-model-selection"><span title="Model Selection and Tradeoffs" class="linkLabel_WmDU">Model Selection and Tradeoffs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/cost-intuition"><span title="Cost Intuition" class="linkLabel_WmDU">Cost Intuition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/cheat-sheet"><span title="Lifecycle Cheat Sheet" class="linkLabel_WmDU">Lifecycle Cheat Sheet</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/discovery-brief"><span title="Discovery Brief" class="linkLabel_WmDU">Discovery Brief</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/the-genai-llm-loop"><span title="The GenAI &amp; LLM Documentation Loop" class="linkLabel_WmDU">The GenAI &amp; LLM Documentation Loop</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/genai-llm-loop-spec"><span title="GenAI &amp; LLM Documentation Loop: Normative Process Model" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Loop: Normative Process Model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/working-agreements-for-teams"><span title="Working Agreements for Teams" class="linkLabel_WmDU">Working Agreements for Teams</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/intent-spec"><span title="Intent Spec" class="linkLabel_WmDU">Intent Spec</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/constraint-spec"><span title="Constraint Spec" class="linkLabel_WmDU">Constraint Spec</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/delegation-contract"><span title="Delegation Contract" class="linkLabel_WmDU">Delegation Contract</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/generation-requests"><span title="Generation Requests" class="linkLabel_WmDU">Generation Requests</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/review-and-interrogation"><span title="Review and Interrogation" class="linkLabel_WmDU">Review and Interrogation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/acceptance-criteria"><span title="Acceptance Criteria" class="linkLabel_WmDU">Acceptance Criteria</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/iteration-and-release"><span title="Iteration and Release" class="linkLabel_WmDU">Iteration and Release</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/accountability-and-delegation"><span title="GenAI &amp; LLM Documentation Accountability and Delegation Model" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Accountability and Delegation Model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/artifact-contracts"><span title="GenAI &amp; LLM Documentation Artifact Contracts (Normative)" class="linkLabel_WmDU">GenAI &amp; LLM Documentation Artifact Contracts (Normative)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/risks-production-challenges"><span title="Risks &amp; Production Challenges" class="linkLabel_WmDU">Risks &amp; Production Challenges</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/prompt-engineering"><span title="Prompt Engineering" class="linkLabel_WmDU">Prompt Engineering</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/instruction-tuning"><span title="Instruction Tuning" class="linkLabel_WmDU">Instruction Tuning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/fine-tuning"><span title="Fine-tuning" class="linkLabel_WmDU">Fine-tuning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/peft"><span title="Parameter-Efficient Fine-tuning (PEFT)" class="linkLabel_WmDU">Parameter-Efficient Fine-tuning (PEFT)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/05-rag"><span title="Retrieval Augmented Generation (RAG)" class="linkLabel_WmDU">Retrieval Augmented Generation (RAG)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/07-tool-use-and-agents"><span title="Tool Use and Agents" class="linkLabel_WmDU">Tool Use and Agents</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/testing-tools"><span title="Testing Tools" class="linkLabel_WmDU">Testing Tools</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/evaluation"><span title="Evaluation" class="linkLabel_WmDU">Evaluation</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/core-skills/00-core-skills-overview"><span title="Core Skills" class="categoryLinkLabel_W154">Core Skills</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/gen-ai-llm-docs/docs/01-handbook-core-method/08-evaluation/00-eval-overview"><span title="Evaluation Library" class="categoryLinkLabel_W154">Evaluation Library</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/02-execution-patterns/00-pattern-index"><span title="Execution Patterns" class="categoryLinkLabel_W154">Execution Patterns</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/03-professional-scenarios/00-scenarios-index"><span title="Professional Scenarios" class="categoryLinkLabel_W154">Professional Scenarios</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/05-tooling-and-frameworks/00-tooling-index"><span title="Tooling &amp; Frameworks" class="categoryLinkLabel_W154">Tooling &amp; Frameworks</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/04-responsible-ai/01-accountability-and-delegation"><span title="Responsible AI" class="categoryLinkLabel_W154">Responsible AI</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/gen-ai-llm-docs/docs/06-templates/00-templates-index"><span title="Templates" class="categoryLinkLabel_W154">Templates</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/AGENTS"><span title="CLI Agents (General)" class="linkLabel_WmDU">CLI Agents (General)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/CHANGELOG"><span title="Changelog" class="linkLabel_WmDU">Changelog</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/gen-ai-llm-docs/docs/LICENSE"><span title="License" class="linkLabel_WmDU">License</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/gen-ai-llm-docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/gen-ai-llm-docs/docs/foundations/01-generative-ai-introduction/introduction"><span>Foundations</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Attention &amp; Transformers</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Attention Mechanism</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Attention Mechanism</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>The Solution to the Bottleneck: Giving the Model &quot;Eyes&quot;</div><div class="admonitionContent_BuS1"><p>In the last section, we saw that the Encoder-Decoder model has a major flaw: it forces all information into a single &quot;bottleneck&quot; context vector. The decoder is like a writer who was given a one-paragraph summary and then had the original book taken away.</p><p><strong>Attention</strong> is the revolutionary idea that solves this. It gives the decoder &quot;eyes&quot; to look back at the <em>entire</em> source text at every step of the writing process. It allows the model to ask, &quot;Which part of the original input is most relevant to the word I&#x27;m about to write?&quot; This simple but powerful idea changed everything.</p></div></div>
<p>The <strong>Attention Mechanism</strong> is one of the most significant innovations in deep learning for sequence modeling, particularly in Natural Language Processing (NLP). Introduced primarily to address the limitations of <a class="" href="/gen-ai-llm-docs/docs/foundations/04-sequential-models/encoder-decoder-model">Encoder-Decoder models with RNNs</a>, Attention allows a model to &quot;pay attention&quot; to (or weigh the importance of) different parts of an input sequence when generating an output. This dynamic focusing dramatically improved performance in tasks like machine translation and laid the foundation for the Transformer architecture.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-problem-with-fixed-size-context-vectors">The Problem with Fixed-Size Context Vectors<a href="#the-problem-with-fixed-size-context-vectors" class="hash-link" aria-label="Direct link to The Problem with Fixed-Size Context Vectors" title="Direct link to The Problem with Fixed-Size Context Vectors" translate="no">​</a></h2>
<p>Recall that in a traditional RNN-based Encoder-Decoder model, the encoder compresses the entire input sequence into a single, fixed-size <strong>context vector</strong>. This context vector then serves as the sole source of information for the decoder to generate the output sequence.</p>
<ul>
<li class=""><strong>Information Bottleneck</strong>: For long input sequences, this fixed-size context vector becomes an information bottleneck. It&#x27;s difficult to encode all relevant details of a long sentence into a single vector, leading to information loss, especially for earlier parts of the input.</li>
<li class=""><strong>Difficulty with Long-Range Dependencies</strong>: As the sequence length increases, the model struggles to remember and utilize information from distant past steps.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-attention-works-conceptually">How Attention Works (Conceptually)<a href="#how-attention-works-conceptually" class="hash-link" aria-label="Direct link to How Attention Works (Conceptually)" title="Direct link to How Attention Works (Conceptually)" translate="no">​</a></h3>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Analogy: The Human Translator with a Highlighter</div><div class="admonitionContent_BuS1"><p>Imagine a human translating the French sentence: &quot;Je suis étudiant.&quot;</p><ul>
<li class="">When translating &quot;I&quot;, they will pay high attention to &quot;Je&quot;.</li>
<li class="">When translating &quot;am&quot;, they will pay high attention to &quot;suis&quot;.</li>
<li class="">When translating &quot;student&quot;, they will pay high attention to &quot;étudiant&quot;.</li>
</ul><p>The attention mechanism works like this. For each word it generates, the decoder creates a &quot;highlighter&quot; (the attention weights) that emphasizes the most relevant words from the original input. The final context vector is a blend of the input words, with the highlighted words given the most importance.</p></div></div>
<p>Instead of forcing the encoder to compress everything into one vector, the Attention Mechanism allows the decoder to directly access all of the encoder&#x27;s hidden states (or a weighted sum of them) when producing each part of the output.</p>
<p>Here&#x27;s the conceptual breakdown:</p>
<ol>
<li class=""><strong>Encoder States</strong>: The encoder processes the input sequence and produces a series of hidden states, one for each input token. These states represent information about the input at different points in time.</li>
<li class=""><strong>Decoder Query</strong>: When the decoder is about to produce an output token, it generates a &quot;query&quot; (typically its current hidden state).</li>
<li class=""><strong>Alignment/Scoring</strong>: This query is compared against all of the encoder&#x27;s hidden states. A &quot;score&quot; (or &quot;attention weight&quot;) is calculated for each encoder state, indicating how relevant that part of the input sequence is to the current decoding step.</li>
<li class=""><strong>Context Vector Creation</strong>: These attention weights are normalized (e.g., using a softmax function) to sum to 1. A new, dynamic <strong>context vector</strong> is then created as a weighted sum of the encoder&#x27;s hidden states, where the weights are the calculated attention scores.</li>
<li class=""><strong>Output Generation</strong>: The decoder uses this new, dynamically created context vector (which is tailored to the current output step) along with its current hidden state to predict the next output token.</li>
</ol>
<p><strong>Actionable Insight</strong>: Attention allows the model to selectively focus on the most relevant input information, effectively creating a &quot;shortcut&quot; for information flow and bypassing the bottleneck of a single fixed-size context vector.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="visual-suggestion-attention-mechanism">Visual Suggestion: Attention Mechanism<a href="#visual-suggestion-attention-mechanism" class="hash-link" aria-label="Direct link to Visual Suggestion: Attention Mechanism" title="Direct link to Visual Suggestion: Attention Mechanism" translate="no">​</a></h2>
<!-- -->
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-attention">Types of Attention<a href="#types-of-attention" class="hash-link" aria-label="Direct link to Types of Attention" title="Direct link to Types of Attention" translate="no">​</a></h2>
<p>While the core concept remains, various &quot;flavors&quot; of attention have been developed:</p>
<ul>
<li class="">
<p><strong>Additive Attention (Bahdanau Attention)</strong>: Uses a feedforward network to calculate alignment scores.</p>
</li>
<li class="">
<p><strong>Multiplicative Attention (Luong Attention)</strong>: Computes scores as a dot product between hidden states.</p>
</li>
<li class="">
<p><strong>Self-Attention (Intra-Attention)</strong>: A revolutionary variant where attention is applied within a single sequence to relate different positions of that same sequence, rather than between an input and an output sequence. This is the cornerstone of the Transformer architecture.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>The Most Important Innovation: Self-Attention</div><div class="admonitionContent_BuS1"><p>This is the big one. While previous attention mechanisms connected a decoder to an encoder, <strong>self-attention</strong> allows a model to look at other words <em>within the same sentence</em> to better understand the context of a given word.</p><p>For example, in the sentence &quot;The animal didn&#x27;t cross the street because <strong>it</strong> was too tired,&quot; self-attention allows the model to learn that &quot;<strong>it</strong>&quot; refers to &quot;the animal&quot; and not &quot;the street.&quot; This ability to resolve relationships within a single sequence is fundamental to how Transformers work and was a massive leap forward for language understanding.</p></div></div>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="relevance-to-generative-ai-and-llms">Relevance to Generative AI and LLMs<a href="#relevance-to-generative-ai-and-llms" class="hash-link" aria-label="Direct link to Relevance to Generative AI and LLMs" title="Direct link to Relevance to Generative AI and LLMs" translate="no">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>The Enabling Technology for Modern LLMs</div><div class="admonitionContent_BuS1"><p>It is impossible to overstate the importance of Attention.</p><ul>
<li class="">It solved the long-range dependency problem that plagued RNNs.</li>
<li class="">Its <strong>self-attention</strong> variant allowed for massive parallelization, as the model could process all words in a sequence at once, rather than one by one.</li>
</ul><p>Without the Attention Mechanism, the Transformer architecture would not exist. And without Transformers, the large, powerful LLMs we have today would not be computationally feasible. Attention is the engine of the modern AI revolution.</p></div></div>
<p>The Attention Mechanism was a game-changer. It directly addressed the long-standing problem of learning long-range dependencies in sequence models. Its ability to dynamically weigh input information significantly improved the quality of generated text and translations.</p>
<p>The most profound impact of Attention was its role in enabling the <strong>Transformer architecture</strong>. By allowing models to process all parts of a sequence in parallel (instead of sequentially like RNNs) and integrating self-attention, Transformers achieved unprecedented efficiency and performance, leading directly to the development of modern LLMs like BERT, GPT, and their successors.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">​</a></h2>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="with-attention-as-a-foundational-concept-we-are-now-ready-to-dive-into-the-revolutionary-transformer-model-architecture">With Attention as a foundational concept, we are now ready to dive into the revolutionary <a class="" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture">Transformer Model Architecture</a>.<a href="#with-attention-as-a-foundational-concept-we-are-now-ready-to-dive-into-the-revolutionary-transformer-model-architecture" class="hash-link" aria-label="Direct link to with-attention-as-a-foundational-concept-we-are-now-ready-to-dive-into-the-revolutionary-transformer-model-architecture" title="Direct link to with-attention-as-a-foundational-concept-we-are-now-ready-to-dive-into-the-revolutionary-transformer-model-architecture" translate="no">​</a></h2></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/gen-ai-llm-docs/docs/foundations/04-sequential-models/beam-search-and-bleu-evaluation-matrices"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Beam Search and BLEU Evaluation Metrics</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/gen-ai-llm-docs/docs/foundations/05-attention-and-transformers/transformer-model-architecture"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Transformer Model Architecture</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-problem-with-fixed-size-context-vectors" class="table-of-contents__link toc-highlight">The Problem with Fixed-Size Context Vectors</a><ul><li><a href="#how-attention-works-conceptually" class="table-of-contents__link toc-highlight">How Attention Works (Conceptually)</a></li></ul></li><li><a href="#visual-suggestion-attention-mechanism" class="table-of-contents__link toc-highlight">Visual Suggestion: Attention Mechanism</a></li><li><a href="#types-of-attention" class="table-of-contents__link toc-highlight">Types of Attention</a></li><li><a href="#relevance-to-generative-ai-and-llms" class="table-of-contents__link toc-highlight">Relevance to Generative AI and LLMs</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li><li><a href="#with-attention-as-a-foundational-concept-we-are-now-ready-to-dive-into-the-revolutionary-transformer-model-architecture" class="table-of-contents__link toc-highlight">With Attention as a foundational concept, we are now ready to dive into the revolutionary Transformer Model Architecture.</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">GenAI & LLM Handbook · 2025</div></div></div></footer></div>
</body>
</html>